----------------------- REVIEW 1 ---------------------

----------- Overall evaluation -----------
Despite the clear improvement over baseline, the percentage gained does not appear
to be too big. It is possible, that the potential effect of better orderings is not
too high, regardless of the optimizations. Can you give some insights for this
line of thought?  We should also hope for further improvements on the basis of
the impressive work presented.
FB: Let's ignore this comment.

Although a paper does a good job of presenting a lot of complex details
in a limited-size paper, a few sections would benefit from further clarification,
for example:

"Each layer consists of a collection of differentiable modules--one module per edge type." 
Are there just so many nodes in the layer as the types of graph edges in Figure 2, 
or is each edge in the whole graph (a large set) represented by a collection of modules?
FB: Let's ignore this comment.


----------------------- REVIEW 2 ---------------------

----------- Overall evaluation -----------
  This paper does a good job at presenting both superposition theorem proving and GCN learning at an abstract enough
  level that researchers in both domains can get an idea of what is going on in the other domain. 
  The process of training the GCN is also clearly described. Moreover, the gain in performance is substantial which,
  to my knowledge, is a first as far as ML-based techniques for theorem proving are concerned. 
  For all these reasons, I believe this paper should be accepted at CADE 2021, 
  although the analysis of the key patterns used by the trained recommender (only hinted at as a future work in the conclusion)
  would have been much more interesting in my opinion.
FB: Let's ignore this comment.

  Detailed comments
  -----------------

 - page 4: Why present only the feedforward NN in sect. 2.3 when you are using a GCN? You could at least include a paragraph about the main differences between these two models at the end of 2.3. Then 3.2 could focus on the aspects of GCN that are specific to this particular problem.
   FB: This would probably take a lot of effort. Let's do this if there's some spare time.
 - p11: the reference for Vampire ([20]) describes version 2.6 of Vampire and you are using version 4.3. If available, something more recent would give a better idea of the current capacities of Vampire.
   FB: [20] is recommended here: https://vprover.github.io/usage.html
       Is there a newer summary text?
 - p11: Did you fix the strategy and heuristics used by Vampire throughout the training and experiments or was it run in portfolio mode? What about AVATAR, was it activated?
   FB: Let's include more detail on the configuration in the paper.
 - p12: To clarify the last paragraph of the page, that introduces concepts with which people outside of ML wouldn't be familiar with, I suggest to use columns to show that some sentences are definitions for the terms introduced in the previous sentences, in particular, "... and 0 otherwise. Adaptive sampling..." -> "... and 0 otherwise: Adaptive sampling..." and "... with a parallelizing relaxation. In each iteration..." -> "with a parallelizing relaxation: In each iteration..."
   FB: Let's ignore this comment.
 - p13: Same comment as just above: "... Adam optimizer [15]. Learning rate..." -> "... Adam optimizer [15]: Learning rate..."
   FB: Let's ignore this comment.
 - p14: In your PAAR paper [7], the experimental results show that your linear model is not as good as the baseline but it is no longer the case here. How do you explain this? If it was further improved after PAAR, it should be mentioned.
   FB: Let's ignore this comment.

  Minor comments
  --------------

 - p16-17: The titles of some articles (mostly the arXiv ones, book titles are a different matter) are capitalized, while the others are not. This could be made uniform.
   FB: Let's clean up the bibliography.

----------- Specific questions for rebuttal -----------

 - How does your tool behave in the presence of theories (assuming AVATAR is used)?
 - Now Vampire can also handle higher-order problems. Can your technique also improve Vampire's performance in this context?
 - How long did it take to do all the training?
 - What kind of new problems were solved in terms of the rating of the problems?
 - How independent are the training and test sets, given that the problems in TPTP are grouped by families that may include extremely similar problems?

FB: Let's ignore all of these comments, since they require additional research.


----------------------- REVIEW 3 ---------------------

----------- Overall evaluation -----------
  Detailed comments:

  - p1 (and elsewhere): Please check the paper for proper use of
    articles. In the case of superposition, it has "native support",
    not "a native support". There are similar problems (too many or
    too few articles) throughout the paper.
    FB: Let's do this using Grammarly and Writefull.
  - p3: The KBO allows both partial orderings and quasi-orderings for
    the precedence, so describing a precedence as a permutation of the
    symbol sequence is not, in general possible. Some implementations
    only support total orderings, but that is not a theoretical
    requirement, and should be made explicit.
    FB: Let's improve the text.
  - p4/3 Architecture: Why do you use the number or iterations of the
    main loop? It's quite typical for calculus refinements to make the
    search space narrower, but deeper. In other words, a good
    precedence my well requite more iterations of the main loop and
    still find a proof much faster, because each iteration comprises
    much less work.
    You don't describe how many "incomplete" precedence pairs you
    have, e.g. pair where one precedence leads to a proof, the other
    to a timeout. This may be a more significant source of good
    training cases than the number of main loop iterations if both
    terminate within the time limit.
    FB: Let's ignore this comment.
  - p13: "Next, both training and validation sets were restricted to
    problems whose graph representation consisted of at most 100 000
    nodes to limit the memory requirements of the training." If I
    understand you correctly, the graph representation of a problem is
    fixed. So why don't you filter before generating training
    examples? CPU time must be cheap! ;-)
    FB: Let's ignore this comment. See the rebuttal.
  - p16: The paper is technically over length, but only by the
    acknowledgements section (and would have the same number of total
    pages without it).
    FB: Let's shorten the paper so that even the acknowledgements fit within the limit. The email from the organizers says that the limit is strict, and that it excludes references (so I assume it includes everything else).
