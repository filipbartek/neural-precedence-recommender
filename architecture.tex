% !TEX root = main.tex

A \emph{symbol precedence recommender} is a system that takes
a \gls{cnf} problem $P = (\Sigma, \mathit{Cl})$ as the input,
and produces a precedence $\pi^*$ over the symbols $\Sigma$ as the output.
For the recommender to be useful, it should produce a precedence
that likely leads to a quick search for a proof. In this work,
we use the number of iterations of the saturation loop as a % (reproducible)
metric describing the effort required to find a proof.

The recommender described in this section
first uses a neural network to compute a cost value for each symbol of the input problem,
and then orders the symbols by their costs in a non-increasing order.
In this manner, the task of finding good precedences is reduced to the task
of training a good symbol cost function,
as discussed in \cref{sec:training}.
%The architecture is designed so that the recommender can be trained
%on the results of \gls{atp} executions on various problems with random precedences.
%Since the recommender contains a neural network,
%it is parameterized by weight tensors
%whose values can be trained by gradient descent.

The recommender consists of modules that perform specific sub-tasks,
each of which is described in detail in one of the following\iftoggle{LONG}{
sections (see also \cref{fig:architecture}):
\begin{itemize}
\item \Cref{sec:graphifier}: The graph constructor transforms the input \gls{cnf} problem into a problem graph.
\item \Cref{sec:gcn}: The \gls{gcn} computes symbol embeddings from the problem graph.
\item \Cref{sec:output}: The output layer extracts symbol costs from the symbol embeddings.
\item \Cref{sec:sorting}: The final symbol precedence is obtained by sorting the symbols by their costs.
\end{itemize}
}{
sections (see also \cref{fig:architecture}).
}

\begin{figure}[ht]
\centering
\digraph[scale=0.5]{ArchitectureOverview}{
graph [splines=ortho, ranksep=0.25];
node [shape=box, fontsize=14, width=0, height=0];

{ rank = same;
pi [label=<Precedence &pi;>];
rho [label=<Precedence &rho;>];
}

Problem [label="Problem P"];

{ rank = same;
Graphifier [style=rounded, label="Graph constructor"];
g [label="Graph"];
GCN [style=<rounded,bold>, label="GCN"];
SymbolEmbedding [label="Symbol embeddings"];
SymbolCostModel [style=<rounded,bold>, label="Output layer"];
}

SymbolCost [label="Symbol costs"];

Sort [style=rounded, label="Sort"];
LossFunction [style=rounded, label="Loss function"];

{ rank = same;
Precedence [label=<Precedence &pi;*>];
Loss [label="Loss value"];
}

Problem -> Graphifier -> g -> GCN;
GCN -> SymbolEmbedding -> SymbolCostModel -> SymbolCost [penwidth=3];
SymbolCost -> Sort;
Sort -> Precedence;

pi -> LossFunction:nw;
rho -> LossFunction:ne;
SymbolCost -> LossFunction [penwidth=3];
LossFunction -> Loss [penwidth=3];
}

\iffalse

\usetikzlibrary{shapes}
\tikzstyle{object} = [rectangle, draw]
\tikzstyle{input} = [ellipse, draw]
\tikzstyle{output} = [ellipse, draw]

\begin{tikzpicture}[node distance = 0.5 and 1, ->]
% https://tex.stackexchange.com/a/332796/202639

\node (problem) [input] {\gls{cnf} problem $P$};
\node (symbol embeddings) [object] [below=of problem] {Symbol embeddings};
\node (symbol costs) [object] [below=of symbol embeddings] {Symbol costs};
\node (symbol precedence) [output] [below=of symbol costs] {Symbol precedence};

\draw (problem) to node [left] {GCN} (symbol embeddings);
\draw (symbol embeddings) to node [left] {MLP} (symbol costs);
\draw (symbol costs) to node [left] {Order symbols by their costs} (symbol precedence);

\node (PrecBetter) [input] [right=of problem] {Precedence $\PrecBetter$};
\node (PrecBetterInv) [object] [below=of PrecBetter] {$\inv{\PrecBetter}$};
\draw (PrecBetter) to node [right] {Invert} (PrecBetterInv);

\node (PrecWorse) [input] [right=of PrecBetter] {Precedence $\PrecWorse$};
\node (PrecWorseInv) [object] [below=of PrecWorse] {$\inv{\PrecWorse}$};
\draw (PrecWorse) to node [right] {Invert} (PrecWorseInv);

\node (PrecDiff) [object] [right=2 of symbol costs] {$\inv{\PrecWorse} - \inv{\PrecBetter}$};
\node (PrecDiffNormalized) [object] [below=of PrecDiff] {Normalized};
\node (PrecPairCost) [object] [below=of PrecDiffNormalized] {Precedence pair cost};
\node (loss) [output] [below=of PrecPairCost] {Loss};

\draw (PrecBetterInv) to (PrecDiff);
\draw (PrecWorseInv) to (PrecDiff);
\draw (PrecDiff) to node [right] {Normalize} (PrecDiffNormalized);
\draw (PrecDiffNormalized) to (PrecPairCost);
\draw (PrecPairCost) to (loss);

\draw (symbol costs) to (PrecPairCost);

\end{tikzpicture}

\fi

\caption{Recommender architecture overview.
When recommending a precedence, the input is problem $P$ and the output is precedence $\pi^*$.
When training, the input is problem $P$ and precedences $\pi$ and $\rho$,
and the output is the loss value.
The trainable modules and the edges along which the loss gradient is propagated are emphasized by bold lines.}
\label{fig:architecture}
\end{figure}
\todo{FB: Improve the diagram. Consider using tikz. See the disabled tikz figure in the source file.}

\subsection{\titlecap{Graph constructor: From \glsentryshort{cnf} to graphs}}
\label{sec:graphifier}

As the first step of the recommender processing pipeline,
the input problem is converted from a \gls{cnf} representation
to a \emph{heterogeneous (directed) graph} \cite{Zhou2018}.
% Zhou uses the term "heterogeneous graph".
%\footnote{Data mining literature often uses the term \gls{hin} \cite{Shi2015} for heterogeneous graphs.
%We prefer to conform to the terminology common in literature studying \glspl{gnn}.}
Each of the nodes of the graph is labeled with a node type,
and each edge is labeled with an edge type,
defining the heterogeneous nature of the graph.
Each node corresponds to one of the elements that constitute the \gls{cnf} formula,
such as a clause, an atom, or a predicate symbol.
Each such category of elements corresponds to one node type.
The edges represent the (oriented) relations between the elements,
for example, the incidence relation between a clause and one of its (literals') atoms,
or the relation between an atom and its predicate symbol.
$\mathcal{R}$ denotes the set of all relations in the graph.
\Cref{fig:CnfSchema} shows the types of nodes and edges used in our graph representation.
\Cref{fig:GcnExample} shows an example of a graph representation of a simple problem.

\newcommand{\ntype}[1]{\texttt{#1}}
\newcommand{\etype}[1]{\texttt{#1}}
\newcommand{\epos}{\etype{pos}}
\newcommand{\eneg}{\etype{neg}}

\begin{figure}[ht]
\centering
\digraph[scale=0.5]{CnfSchema}{
graph [ranksep=0.3];
node [fontsize=14, shape=box, height=0, width=0];
edge [fontsize=14];

problem;
clause;
%{ rank = same;
predicate;
atom [label="non-equality atom"];
equality [label="equality atom"];
%}
argument;
term;
function;
variable;

problem -> clause [label=< contains >];
clause -> atom:nw [label=< pos >];
clause -> atom:ne [label=< neg >];
clause -> equality:nw [label=< pos >];
clause -> equality:ne [label=< neg >];
clause -> variable [label=< binds >];
atom -> predicate [label=< atom_applies>];
atom -> argument [label=< atom_has >];
equality -> term [label=< equalizes >];
equality -> variable [label=< equalizes >];
argument -> argument [label=< precedes >];
argument -> term [label=< is >];
argument -> variable [label=< is >];
term -> argument [label=< term_has >];
term -> function [label=< term_applies >];
}

\iffalse
\tikzstyle{token} = [rectangle, draw]

\begin{tikzpicture}[node distance = 1 and 2, ->]
% https://tex.stackexchange.com/a/332796/202639

% Node and edge types:
% https://docs.google.com/spreadsheets/d/1PCPHEgk6vLxpdpcvB_PGoLx7p4DID6WtvVWy2mDuv4A/edit?usp=sharing

\node (formula) [token] {\ntype{problem}};
% TODO: Consider removing the Formula nodes.
\node (clause) [token, below=of formula] {\ntype{clause}};
\node (atom) [token, below left=of clause] {\ntype{atom}};
\node (equality) [token, below right=of clause] {\ntype{equality}};
\node (predicate) [token, left=of atom] {\ntype{predicate}};
\node (argument) [token, below=of atom] {\ntype{argument}};
\node (term) [token, below=of argument] {\ntype{term}};
\node (function) [token, left=of term] {\ntype{function}};
\node (variable) [token, right=of term] {\ntype{variable}};

\draw (formula) to node [right] {\etype{contains}} (clause);
\draw (clause) to [bend right] node [above] {\epos{}} (atom);
\draw (clause) to [bend left] node [below] {\eneg{}} (atom);
\draw (clause) to [bend left] node [above] {\epos{}} (equality);
\draw (clause) to [bend right] node [below] {\eneg{}} (equality);
\draw (clause) to node [above] {\etype{binds}} (variable);
\draw (atom) to node [above] {\etype{atom\_applies}} (predicate);
\draw (atom) to node [left] {\etype{atom\_has}} (argument);
\draw (equality) to node {\etype{equalizes}} (term);
\draw (equality) to node {\etype{equalizes}} (variable);
\draw (argument) to [bend right] node [left] {\etype{is}} (term);
\draw (argument) to [loop left] node [left] {\etype{precedes}} (argument);
\draw (argument) to node [below] {\etype{is}} (variable);
\draw (term) to node [above] {\etype{term\_applies}} (function);
\draw (term) to [bend right] node [right] {\etype{term\_has}} (argument);

\end{tikzpicture}
\fi

\caption{\gls{cnf} graph schema}
\label{fig:CnfSchema}
\end{figure}
\todo{FB: Clean up the diagram.}

\begin{figure}[ht]
\centering
\digraph[scale=0.5]{GcnExample}{
graph [ranksep=0.3];
node [fontsize=14, shape=record, height=0, width=0];
edge [fontsize=14, dir=both, arrowtail=empty];

problem [label=<problem|a=b &and; f(a,b)&ne;f(b,b)>];

{ rank = same;
c0 [label="clause|a=b"];
c1 [label=<clause|f(a,b)&ne;f(b,b)>];
}

problem -> c0;
problem -> c1;

{ rank = same;
t0 [label="equality atom|a=b"];
t1 [label="equality atom|f(a,b)=f(b,b)"];
}

c0 -> t0 [label=" pos "];
c1 -> t1 [label=" neg "];

{ rank = same;
tfab [label="term|f(a,b)"];
tfbb [label="term|f(b,b)"];
}

{ rank = same;
ta [label="term|a"];
tb [label="term|b"];
}

ff [label="function|f"];
fa [label="function|a"];
fb [label="function|b"];
tfab0 [label="argument|1"];
tfab1 [label="argument|2"];
tfbb0 [label="argument|1"];
tfbb1 [label="argument|2"];
t0 -> ta;
t0 -> tb;
t1 -> tfab;
t1 -> tfbb;
tfab -> ff;
tfab -> tfab0;
tfab0 -> tfab1;
tfab0 -> ta;
tfab1 -> tb;
tfbb -> ff;
tfbb -> tfbb0;
tfbb0 -> tfbb1;
tfbb0 -> tb;
tfbb1 -> tb;
ta -> fa;
tb -> fb;

% Technical details:
%ff -> ff [dir=forward];
%fa -> fa [dir=forward];
%fb -> fb [dir=forward];
}
\caption{Graph representation of the \gls{cnf} formula $a=b \wedge f(a,b) \neq f(b,b)$}
\label{fig:GcnExample}
\end{figure}

The graph representation exhibits, namely, the following properties:
\begin{itemize}
\item Lossless: The original problem can be faithfully reconstructed from the corresponding graph representation
(up to logical equivalence).
\item Signature agnostic: Renaming the symbols and variables in the input problem yields an isomorphic graph.
\item For each relation $r \in \mathcal{R}$, its inverse $\inv{r}$ is also present in the graph,
typically\todo{MS: jaka je vyjimka?} represented by a different edge type.
\todo{MS: tohle jsem nepochopil. Nemuzeme rict, ze (az na nejakou vyjimkou) jsou edges v obou smerech?\\
FB: Chci vyjasnit, ze inverzni edge ma jiny edge type.}
%\item A singleton node of type \ntype{problem} is connected to all the clauses of the problem.
\item The polarity of the literals is expressed by the type of the edge (\epos{} or \eneg{})
connecting the respective atom to the clause it occurs in.
\item For every non-equality atom and term, the order of its arguments is captured by a sequence of \ntype{argument} nodes chained by edges \cite{Rawson2020}.
\item The two operands of equality are not ordered.
This reflects the symmetry of equality.
\item Sub-expression sharing \cite{Chvalovsky2019,Olsak2019,Rawson2020}:
\todo[inline]{MS: (Almost)}
\todo[inline]{MS: Predpokladam ze dva ruzne ground literaly p(c) tam budou taky jen jendnou?\\
FB: Literaly nemaji nody. Atomy maji nody a polarita literalu je zakodovana v typu hrany, ktery jej spojuje s klauzuli.
MS: O to my, neslo. Slo o to, jestli se sdily ground termy mezi klauzulemi, ted uz dobry! :)}
Identical atoms and terms share a node representation.
\todo{FB: Cite some text about sub-expression sharing. Perhaps Rawson2020?}
\todo{MS: Ale mezitim se to tu zmenilo a ja prestavam chapat, co je ``Redundant''!}
%Note that since each variable is bound by a clause,
%ground terms are shared across clauses,
%but non-ground terms are only shared within the context of a clause.
\todo{FB: Remove as of little interest?}
\end{itemize}
\todo{FB: Reference appendix if we describe the representation in more detail there.}
\todo{FB: Mention other encodings that have been proposed by Mirek and Michael and compare ours to theirs.}

\subsection{\titlecap{\glsentryshort{gcn}: From graphs to symbol embeddings}}
\label{sec:gcn}

For each symbol in the input problem $P$,
we seek to find a vector representation, i.e., an \emph{embedding},
that captures the symbol's properties that are relevant
for correctly ranking the symbol in the symbol precedences over $P$.

The symbol embeddings are output by a \emph{\gls{rgcn}} \cite{Schlichtkrull2017},
which is a stack of \emph{graph convolutional layers}.
Each layer consists of a collection of differentiable modules---one module per edge type.
The computation of the \gls{gcn} starts with assigning each node an initial embedding
and then iteratively updates the embeddings by passing them through the convolutional layers.

The initial embedding $h_a^{(0)}$ of a node $a$ is a concatenation of two vectors:
a \emph{feature vector} specific for that node (typically empty)
and a trainable vector shared by all nodes of the same type.
In our particular implementation,
feature vectors are used in nodes that correspond to clauses and symbols.
Each clause node has a feature vector with a one-hot encoding of the role of the clause,
which can be either axiom, assumption, or negated conjecture \cite{TptpSyntax,Sutcliffe2017}.
Each symbol node has a feature vector with two bits of data:
whether the symbol was introduced into the problem during preprocessing (most notably during clausification),
and whether the symbol appears in a conjecture clause.

One pass through the convolutional layer
updates the node embeddings by passing a message along each of the edges.
For an edge of type $r \in \mathcal{R}$ going from source node $s$ to destination node $d$ at layer $l$,
the message is composed by converting the embedding of the source node $h_s^{(l)}$
using the module associated with the edge type $r$.
In the simple case that the module is a fully connected layer with weight matrix $W_r^{(l)}$ and bias vector $b_r^{(l)}$,
the message is $W_r^{(l)} h_s^{(l)} + b_r^{(l)}$.
% Notation is from Kipf: RGCN paper.
Each message is then divided by the normalization constant
$c_{s,d} = \sqrt{\card{\mathcal{N}_s^r}} \sqrt{\card{\mathcal{N}_d^r}}$ \cite{kipf2017semisupervised},
where $\mathcal{N}_a^r$ is the set of neighbors of node $a$ under the relation $r$.
\todo{FB: Mention that we mean both in- and out-neighbors?}

Once all messages are computed,
they are aggregated at the destination nodes to form new node embeddings.
Each node $d$ aggregates all the incoming messages of a given edge type $r$ by summation,
then passes the sum through an activation function $\sigma$ such as the \gls{relu},
and finally aggregates the messages across the edge types by summation,
yielding the new embedding $h_d^{(l+1)}$.

The following formula captures the complete update of the embedding of node $d$ by layer $l$:
$$
h_d^{(l+1)} =
\sum_{r \in \mathcal{R}} \sigma \Parentheses*{\sum_{s \in \mathcal{N}_d^r} \frac{1}{c_{s,d}} (W_r^{(l)} h_s^{(l)} + b_r^{(l)})}
$$

%$$
%h_i^{(l+1)} =
%\mathrm{LayerNorm} \Parentheses*{h_i^{(l)} + \sum_{r \in \mathcal{R}} \sigma \Parentheses*{\sum_{j \in %\mathcal{N}_i^r} \frac{1}{c_{ji}} h_j^{(l)} W_r^{(l)}}}
%$$
% Inspiration: https://ufal.mff.cuni.cz/~straka/courses/npfl114/1920/slides.pdf/npfl114-07.pdf - slide 27 - Transformer
%\todo{FB: Mention inspiration: Transformer. See Straka's slides.}

\todo{Compare our GCN to Michael, Mirek etc.}

\subsection{\titlecap{Output layer: From symbol embeddings to symbol costs}}
\label{sec:output}
% Terminology: Deep Learning Book

The symbol cost of each symbol is computed by passing the symbol's embedding through a linear output unit,
which is an affine transformation with no activation function.

It is possible to use a more complex output layer in place of the linear unit,
e.g., a feedforward network with one or more hidden layers.
Our experiments showed no significant improvement when a hidden layer was added,
likely because the underlying \gls{gcn} learns a sufficiently complex transformation.

Let $\theta$ denote the vector of all parameters of the whole \acrlong{nn} consisting of the \gls{gcn} and the output unit.
Given an input problem $P$ with signature $\Sigma = (s_1, \ldots, s_n)$,
we denote the cost of symbol $s_i$ predicted by the network as $c(i, P; \theta)$.
In the rest of this text,
we refer to the predicted cost of $s_i$ simply as $c(i)$
because the problem $P$ and the parameters $\theta$ are fixed in each respective context.

\subsection{\titlecap{Sort: From symbol costs to precedence}}
\label{sec:sorting}

The symbol precedence heuristics commonly used in the \glspl{atp} sort the symbols by some numeric syntactic property
that is inexpensive to compute,
such as the number of occurrences in the input problem, or the symbol arity.
In our precedence recommender,
we sort the symbols by their costs $c$ produced by the \acrlong{nn} described in \cref{sec:gcn,sec:output}.
An advantage of this scheme is that sorting is a fast operation.

Moreover, as we show in \cref{sec:training}, it is possible
to train the underlying symbol costs by gradient descent.

%Note that a precedence that minimizes $C$
%orders the symbols with the lowest cost as the last, prioritizing them for early inferences.
