We are very grateful for the attention and care the reviewers took when considering our paper.


Response to Review 1
--------------------

- An obvious question is how is this work related to the work and methods used for Enigma?

ENIGMA-NG [9] differs from our neural precedence recommender (NPR) significantly in that it deals with a different task (given clause selection),
it assumes a common signature across the problems
and it uses a recursive neural network (a more restrictive NN architecture).
ENIGMA Anonymous [Jakubuv 2020] uses the graph neural network (GNN) architecture proposed in [25], and the relation of our work to this architecture is outlined in section 6.
In more detail, NPR deals with the task of proposing good symbol precedences, while ENIGMA solves given clause selection (an instance of clause classification).
NPR is trained on a proxy task of precedence pair classification,
while ENIGMA is trained directly on the task of clause classification.
ENIGMA uses a lossy hypergraph encoding,
while we use a lossless graph encoding with only binary edges,
which allows our architecture to match the standard R-GCN as proposed in [31] more closely.

- Despite the clear improvement over baseline, the percentage gained does not appear
to be too big. It is possible, that the potential effect of better orderings is not
too high, regardless of the optimizations. Can you give some insights for this
line of thought?

It is incredibly difficult to estimate in absolute terms, how large an improvement
one could in our setting on TPTP achieve using some form of "perfect orderings".
However, experiments with random precedences [7] showed that the prover perceptibly responds 
to changes of this parameter and that work on a recommender is worthwhile.

Response to Review 2
--------------------

- How does your tool behave in the presence of theories (assuming AVATAR is used)?

Both in the training and in the evaluation, AVATAR was disabled and we didn't consider problems with theories.
Although, in principle, theory symbols could be subject to our recommender, Vampire treats them differently than the rest  
when constructing a precedence and we did not want to be sidetracked by these specificities. 

- Now Vampire can also handle higher-order problems. Can your technique also improve Vampire's performance in this context?

Again, this seems in principle possible, as long as the ordering used by the HO vampire still relies on a precedence,
but we did not investigate the details yet.

- How long did it take to do all the training?

In total, the generation of the training data and the training of the model took approximately 8 days.
More precisely, it took approximately 2 and 4 days to generate the training data for the predicate and function precedence recommender, respectively,
and it took approximately 45 and 20 hours to train the predicate and function precedence recommender, respectively.

- What kind of new problems were solved in terms of the rating of the problems?

A total of 234 new problems were solved by the trained recommender when compared to the baseline heuristic.
Among these 234 problems, the maximum rating is 0.88,
the mean rating is approximately 0.36 and the median rating is 0.36.

- How independent are the training and test sets, given that the problems in TPTP are grouped by families that may include extremely similar problems?

In this experiment, we have not attempted to control nor analyze the similarity between the problems in the training and test sets.
Splitting the problem set by the domain or the source of the problems is, however, an interesting idea.


Response to Review 3
--------------------

- Why do you use the number or iterations of the main loop?

We mainly selected this metric because it is deterministic
and can be measured while other processes are making the computer busy.
Wall-clock time is more sensitive to the compute environment 
and more care needs to be taken to obtain meaningful commeasureable across multiple runs.
However, the point raised by the reviewer is valid and suggests that using wall clock time as the decisive metric,
if implemented well, may increase the performance of the trained system.

- If I understand you correctly, the graph representation of a problem is fixed. So why don't you filter before generating training examples?

We agree that filtering the problems first by the graph representation size would make the procedure clearer.
The procedure described in the paper was used mainly because it allowed cheaper tuning of the hyperparameters that control the conversion to graphs â€“
in most of the experiments, we only need to convert a small part of the training and test problems,
and determining whether a graph representation of a problem has at most 100 000 nodes requires performing the conversion to graph.
Filtering by the graph size before splitting into training and test set, once the hyperparameters are tuned, seems to be a reasonable improvement.
