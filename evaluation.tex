To demonstrate the capacity of the precedence recommender described in \cref{sec:architecture},
a series of experiments was performed.
\todo{MS: vzdycky kdyz to jen trochu jde, snaz se pouzivat cinny rod (a pritomny cas): To demonstrate ..., we performed ... Ale taky tu trochu chybi, co nas ceka dal, ve stylu: In this section, we report ... }
In this section, we describe the design and configuration of the experiments,
and then compare the performance of the trained models to baseline heuristics.

\subsection{Solver}

The empirical evaluation was performed using a modified version of the \gls{atp} Vampire 4.3.0.\cite{10.1007/978-3-642-39799-8_1}
The prover was used to generate the training data and to evaluate the trained precedence recommender.
To generate the training data,
Vampire was modified to output \gls{cnf} representations of the problems
and annotated problem signatures in a machine-readable format.
%in \gls{json} format and annotated problem signatures in \gls{csv} format.\todo{MS: Mentioning JSON or CSV seems to be too low level. ``programmer documatation'' material.}
For the evaluation of the precedences generated by the recommender,
Vampire was modified to allow the user to specify the predicate and function symbol precedences.
\todo[inline]{FB: Add a link to the modified Vampire.}

% All explicit options:
% vampire --encode on --statistics full --time_statistics on --proof off --avatar off --saturation_algorithm discount --age_weight_ratio 10 --literal_comparison_mode predicate --symbol_precedence frequency --time_limit 10

% Notable explicit options:
% vampire --avatar off --saturation_algorithm discount --age_weight_ratio 10 --literal_comparison_mode predicate --symbol_precedence frequency --time_limit 10

% Notable implicit options:
% --term_ordering kbo

\todo[inline,author=FB]{Describe the Vampire options.}
\todo{MS: Literal comparison mode is not a standard thing. Mentioning the option name is again too low level. Explaining the effect (point two) is appropriate, although a bit too vague. To kdyztak opravim ja. 
Popsat presne setup je dulezite, kvuli reprodukovatelnosti, ale ne nutne primo v clanku (budem nejspis stejne dodavat odkaz na nejake reproducibility repo).
Ted zminit AVATAR je nevhodne, pokud nevysvetlis, co to aspon ramcove je. A pokud to budes vysvetlovat, je otazka, proc tim ztracet misto, kdyz to neni bezprostredne relevantni pro dalsi vysvetlovani.}

%For the experiment, Vampire 4.3.0 was modified to convert the problems from the \gls{tptp} format to a representation that allows easy automated processing.
%More precisely, when the modified prover is run in the clausification mode (\texttt{vampire -mode clausify}),
%it outputs the \gls{cnf} form of the problem and the problem signature in formats that allow easy processing in Python.
%The \gls{cnf} clause and term structure of the problem is encoded into the \gls{json} format,
%and the signature along with additional symbol metadata is encoded into the \gls{csv} format.

\subsection{Training data}

The training data consists of examples of the form $(P, \PrecBetter, \PrecWorse)$,
where $P$ is a \gls{cnf} problem and $\PrecBetter, \PrecWorse$ are precedences of symbols of problem $P$
such that out of the two precedences, $\PrecBetter$ yields a proof in fewer iterations of the saturation loop.
\todo{FB: Justify saturation loop iterations as a proxy for success.}
\todo{FB: Reference a detailed explanation of saturation loop iterations.}

Since neither of the common \glspl{sot}, i.e., \gls{kbo} nor \gls{lpo}, ever compares a predicate symbol with a function symbol,
two separate precedences can be considered for each problem:
a predicate precedence and a function precedence.
A predicate precedence recommender is trained separately from a function precedence recommender
to simplify the training process and to isolate the effects of the predicate and function precedences.
This section describes how the training data for training a \emph{predicate} precedence recommender is generated.
Training data for training a function precedence recommender is generated analogously.

\subsubsection{Problem set}

The examples are generated by sampling the \gls{fol} part of the problem library \gls{tptp} v7.4.0.
A total of \num{17053} problems formulated in \gls{fof} and \gls{cnf} are sampled.
The problems formulated in \gls{fof} are converted to \gls{cnf} by Vampire.
% vampire --mode clausify
Let $\ProblemsTptp$ denote the set of all \num{17053} \gls{cnf} problems available for training and evaluation.

\subsubsection{Sampling}

The examples are generated by iterative sampling of $\ProblemsTptp$.
In each iteration, a problem $P \in \ProblemsTptp$ is chosen and Vampire is executed on $P$
with two predicate precedences chosen uniformly from the set of all predicate precedences on $P$.
A function precedence for $P$ is chosen uniformly and used for both executions.
\todo[author=FB]{Why do we sample function precedences randomly?}

The two executions are compared in terms of performance:
predicate precedence $\PrecBetter$ is recognized as better than precedence $\PrecWorse$,
denoted as $\Better{\PrecBetter}{\PrecWorse}$,
if the proof search finishes successfully with $\PrecBetter$ ($\SolverRun{P}{\Prec} = \top$)
and if the number of iterations of the saturation loop with $\PrecBetter$ is smaller than with $\PrecWorse$.
\todo{FB: Use notation for saturation loop iterations if the notation was established earlier. If failure is denoted by $\infty$, we can simply use $<$.}
If one of the two precedences is recognized as better,
the example $(P, \PrecBetter, \PrecWorse)$ is produced,
where $\PrecBetter$ is the better precedence,
and $\PrecWorse$ is the other precedence.

To ensure the efficiency of the sampling, the process is interpreted as an instance of Bernoulli multi-armed bandit problem,
\todo{Cite?}
with the reward of a trial being 1 in case an example is produced, and 0 otherwise.
Adaptive sampling balances
exploring problems that have been tried relatively scarcely and
exploiting problems that have yielded examples relatively often.
For each problem $P \in \ProblemsTptp$,
the generator keeps track of the number of times the problem has been tried $n_P$
and the number of examples generated from that problem $s_P$.
The ratio of $s_P$ to $n_P$ corresponds to the average reward of problem $P$ observed so far.
The problems are sampled using the allocation strategy \acrshort{ucb1} \cite{Auer2002} with a parallelizing relaxation.
In each iteration, the generator samples the problem $P$ that maximizes
$$
\frac{s_P}{n_P} + \sqrt{\frac{2 \ln n}{n_P}}
$$
where $n = \sum_{P \in \ProblemsTptp} n_P$ is the total number of tries on all problems.
The parallelizing relaxation means that the $s_P$ values are only updated once in \num{1000} iterations,
allowing up to \num{2000} parallel solver executions.
\todo{Explain in detail?}

The sampling runs until a total of \num{1000000} examples are generated.
% sftp://cluster.ciirc.cvut.cz/home/bartefil/git/vampire-ml/out/20210108-generate-predicate/questions_generated
For example, while generating \num{1000000} examples for the predicate precedence dataset,
the least explored problem was tried 19 times, and the most exploited problem was tried 504 times.
\num{5349} out of the \num{17053} problems yielded at least one example.
\todo{FB: Remove these two sentences because the give too arbitrary detail?}

\subsubsection{Split}

The problems in $\ProblemsTptp$ are split roughly in half to form the train set and the validation set.
Both training and validation sets are restricted to problems whose graph representation consists of at most \num{100000} nodes
to limit the memory requirements of the training.
The train set is further restricted to problems that correspond to at least one training example.
In total there are \num{7648} problems in the validation set $\ProblemsVal$
and \num{2571} problems in the train set $\ProblemsTrain$.
\todo{FB: Unify the terminology: validation, or train set?}
% /home/filip/projects/vampire-ml/vampire-ml/outputs/2021-02-11/16-27-02/results.csv
% all: 17053
% train: 8527
% val: 8526
% with_questions: 5349
% graphified (not all attempted): 10219
% train&with_questions: 2647
% train&with_questions&graphified: 2571
% val&graphified: 7648
\todo{MS: jeste by nebylo lepsi vzit nejdriv ty ``male'' problemy a pak teprve pulit.}

\subsection{Training procedure}

A predicate symbol cost model is trained by gradient backpropagation
\todo{FB: Explain?}
on the precedence pair classification task
\todo{FB: Reference a section.}
using the examples generated from the problems in $\ProblemsTrain$.
\todo{Add references to the sections that describe the architecture.}
To avoid redundant computations, all examples generated from a problem are processed in the same training batch.
Thus, each training batch contains up to \num{128} problems and all the examples generated from these problems.
The symbol cost model is trained using the Adam optimizer \cite{Kingma2014}.
Learning rate starts at \num{1.28e-3}
and is halved each time the train loss stagnates for 10 consecutive epochs.
% tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10)
A \gls{gcn} with depth 4 and message size 16 is used.\todo{MS: zase pasivum. Tady to urcite bude chtit odkaz na sekci, kde se jasne definuje, co to znamena. Jinak recenzenti byvaji dost alergicti na hyperpametry, ktere se jen tak nahodi na nevysvetli. Minimalne nejaky komentar typu: treba, zkouseli jsme u ruzne dalsi hodnoty, ale dopadalo to podobne. Nebo, pozdeji rekeneme, co se deje, kdyz se tohle meni. Nebo: nemeli jsme cas zjistovat, co se deje, kdyz se to meni, tak berem rozumne hodnoty podle blabla.}

Each of the training examples of problem $P$ contributes to the training with the weight $\frac{1}{s_P}$,
where $s_P$ is the number of examples of problem $P$ in the training set,
so that each problem contributes to the training to the same degree irrespective of the relative numbers of examples.
\todo{FB: Shall we remind that the examples are further normalized by $k(n)$ to make even across signature lengths?}

\subsection{Environment}

All the experiments were run on a computer with the following specification:

\begin{itemize}
\item CPU: Intel Xeon Gold 6140 (72 cores @ \SI{2.30}{GHz})
\item RAM: \SI{383}{GiB}
\item OS: Ubuntu 20.04
\item Kernel: GNU/Linux 5.4.0-40-generic x86\_64
\end{itemize}

% From the paper Reliable benchmarking [BenchExec]:
% – CPU model and size of RAM,
% – specified resource limits,
% – name and version of OS,
% – version of important software packages, such as the kernel or runtime environments like the Java VM,
% – version and configuration of the benchmarked tool(s), and
% – version of the benchmark set (input files)

\subsection{Results}

The final evaluation is performed on $\ProblemsVal$.
For each problem $P \in \ProblemsVal$,
a precedence is generated by the trained recommender
and Vampire is run using this precedence and a wall clock time limit of 10 seconds.
The results are averaged over 5 runs to reduce the effect of noise due to wall clock time limit.
As a baseline, the performance of Vampire with the \texttt{frequency} precedence heuristic was evaluated
with the same time limit.

To generate a precedence for a problem,
the recommender first converts the problem to a machine-friendly \gls{cnf} format,
then converts the \gls{cnf} to a graph,
then predicts symbol costs using the \gls{gcn} model
and finally orders the symbols by their costs to produce the precedence.
To simplify the experiment, the time limit is only imposed on the Vampire run
and excludes the time taken by the recommender to generate the precedence.
\todo{Update if we include the preprocessing time.}

\Cref{tab:results} shows the results.

\begin{table*}
\caption{
Results of the evaluation of various predicate precedence heuristics on $\ProblemsVal$.
Means and standard deviations over 5 runs are reported.
}
\centering
\begin{tabular}{l|ll|ll}

Model & \multicolumn{2}{l}{Successes out of \num{7648}} & \multicolumn{2}{l}{Successes rate} \\
& Mean & Std & Mean & Std \\
\hline

\acrshort{gcn} (predicate) & \num{3923.6} & \num{2.24} & \SI{51.30}{\percent} & \num{2.93e-4} \\
% Success rate: mean: 0.513023013, std: 0.000292887
% Evaluation results: https://ui.neptune.ai/filipbartek/vampire-ml/e/VML-553
% Total: validation_solver_eval/all/problems/measured&split&category: 7648
% Success mean: validation_solver_eval/all/success/count/mean: 3923.6
% Success std: validation_solver_eval/all/success/count/std 2.24
% Success rate: 0.513023013
% Difference in success count from baseline: 154 ~ 0.020135983
% Estimated difference from baseline (estimate on 891 problems): 0.021099888
% Checkpoint: outputs/2021-02-06/14-55-41/tf_ckpts/epoch/weights.00079-0.61.tf VML-540 0.511785

\acrshort{gcn} (function) & ? & ? & ? & ? \\

\texttt{vampire -lcm predicate} & \num{3769.6} & \num{3.07} & \SI{49.29}{\percent} & \num{4.01e-4} \\
% Success rate: mean: 0.492887029, std: 0.000401412
% https://ui.neptune.ai/filipbartek/vampire-ml/e/VML-490
% /home/filip/projects/vampire-ml/vampire-ml/outputs/2021-02-09/12-13-43
% Row: 'val&graphified&solver_eval'
% Problems total: 7648
% Success rate: 0.492887029

\texttt{vampire -lcm standard} & ? & ? & ? & ? \\

\end{tabular}
\label{tab:results}
\end{table*}
\todo{FB: Add links to Neptune runs?}
\todo{FB: Add results for function precedences.}
\todo{FB: Try to evaluate a recommender that uses both predicate and function models.}

\todo{FB: Add discussion.}
\todo{FB: Evaluate on problems larger than 100k nodes.}
