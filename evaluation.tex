% !TEX root = main.tex

To demonstrate the capacity of the trainable precedence recommender described in \cref{sec:architecture},
we performed a series of experiments.
In this section, we describe the design and configuration of the experiments,
and then compare the performance of several trained models to a number of baseline heuristics.

The scripts that were used to generate the training data and to train and evaluate the recommender
are available at the following URL:
\url{https://github.com/filipbartek/vampire-ml/tree/6907ee35}
\todo{FB: Tag the version and link to the tag.}
\todo{FB: Add some wrapper scripts and readme for ease of use.}

\subsection{Environment}

\subsubsection{System}

All the experiments were run on a computer with the following specification:

\begin{itemize}
\item CPU: Intel Xeon Gold 6140 (72 cores @ \SI{2.30}{GHz})
\item RAM: \SI{383}{GiB}
\item OS: Ubuntu 20.04
\item Kernel: 5.4.0-40-generic
\end{itemize}

% From the paper Reliable benchmarking [BenchExec]:
% – CPU model and size of RAM,
% – specified resource limits,
% – name and version of OS,
% – version of important software packages, such as the kernel or runtime environments like the Java VM,
% – version and configuration of the benchmarked tool(s), and
% – version of the benchmark set (input files)

\subsubsection{Solver}

The empirical evaluation was performed using a modified version of the \gls{atp} \Vampire{} 4.3.0 \cite{10.1007/978-3-642-39799-8_1}.
The prover was used to generate the training data and to evaluate the trained precedence recommender.
To generate the training data,
\Vampire{} was modified to output \gls{cnf} representations of the problems
and annotated problem signatures in a machine-readable format.
%in \gls{json} format and annotated problem signatures in \gls{csv} format.\todo{MS: Mentioning JSON or CSV seems to be too low level. ``programmer documatation'' material.}
For the evaluation of the precedences generated by the recommender,
\Vampire{} was modified to allow the user to supply explicit predicate and function symbol precedences for the proof search
(normally, the user only picks a precedence generation heuristic).
The modified version of \Vampire{} is available at the following URL:
\url{https://github.com/filipbartek/vampire/tree/3bbfa9e6}

\todo[inline,author=MS]{Literal comparison mode is not a standard thing. Mentioning the option name is again too low level. Explaining the effect (point two) is appropriate, although a bit too vague. To kdyztak opravim ja. 
Popsat presne setup je dulezite, kvuli reprodukovatelnosti, ale ne nutne primo v clanku (budem nejspis stejne dodavat odkaz na nejake reproducibility repo).
Ted zminit AVATAR je nevhodne, pokud nevysvetlis, co to aspon ramcove je. A pokud to budes vysvetlovat, je otazka, proc tim ztracet misto, kdyz to neni bezprostredne relevantni pro dalsi vysvetlovani.}

\Vampire{} was run with a time limit of 10 seconds.
To increase the potential impact of predicate precedences,
we used a simple \gls{tkbo} \cite{Ludwig2007,Kovacs2011}
that compares atoms according to the predicate precedence first,
using the regular \gls{kbo} to break ties between atoms
and to compare terms.
\todo[inline,author=FB]{Describe the detailed configuration in an appendix and reference the appendix here.}

% All explicit options:
% vampire --encode on --statistics full --time_statistics on --proof off --avatar off --saturation_algorithm discount --age_weight_ratio 10 --literal_comparison_mode predicate --symbol_precedence frequency --time_limit 10

% Notable explicit options:
% vampire --avatar off --saturation_algorithm discount --age_weight_ratio 10 --literal_comparison_mode predicate --symbol_precedence frequency --time_limit 10

% Notable implicit options:
% --term_ordering kbo

\subsubsection{Input problems}

The input problems are assumed to be specified in the \gls{cnf} or the \gls{fof} fragment
of the \acrshort{tptp} language \cite{Sutcliffe2017}.
% http://www.tptp.org/TPTP/SyntaxBNF.html
\todo{FB: Consider joining with section Problem set below.}
\Gls{fof} problems are first converted into equisatisfiable \gls{cnf} problems by \Vampire{}.
New symbols may be introduced during preprocessing,
namely due to the Tseitin transformation and Skolemization.
We denote the preprocessed \gls{cnf} problem as $P = (\Sigma, \mathit{Cl})$,
where $\Sigma$ denotes the list of all predicate and function symbols of the problem,
and $\mathit{Cl}$ denotes a set of clauses over $\Sigma$ that constitute the problem.

In addition to the signature and the structure of the problem,
some metadata is extracted from the input problem to allow training a more efficient recommender.
First, each clause is annotated with its role in the problem,
which can be either axiom, assumption, or negated conjecture.
% This is simplified. There are actually 7 clause roles defined internally in Vampire,
% and it seems that only 4 of them are ever used in FOL problems:
% AXIOM, ASSUMPTION, CONJECTURE, NEGATED_CONJECTURE
% CONJECTURE is used in FOF problems and NEGATED_CONJECTURE is used in CNF problems,
% so the recommender can actually differentiate between these two forms.
Second, each symbol is annotated with two bits of data:
whether it was introduced into the problem during preprocessing,
and whether the symbol appears in a conjecture clause.
% Note that the second bit is redundant, since the clauses are annotated with their roles.
This metadata is used to construct the initial embeddings of the respective nodes
in the graph representation of the problem (see \cref{sec:gcn}).
\todo{FB: Consider simplifying the paragraph by removing overlap with \cref{sec:gcn}.}

\subsection{Training data}

The training data consists of examples of the form $(P, \PrecBetter, \PrecWorse)$,
where $P$ is a \gls{cnf} problem and $\PrecBetter, \PrecWorse$ are precedences of symbols of problem $P$
such that out of the two precedences, $\PrecBetter$ yields a proof in fewer iterations of the saturation loop (see \cref{sec:saturation}).
\todo{FB: Justify saturation loop iterations as a proxy for success. MS: Ano, ale driv nez zde!}

Since the \gls{tkbo} never compares a predicate symbol with a function symbol,
\todo{FB: Consider referencing an earlier section that explains KBO or TKBO.}
two separate precedences can be considered for each problem:
a predicate precedence and a function precedence.
We trained a predicate precedence recommender separately from a function precedence recommender
to simplify the training process and to isolate the effects of the predicate and function precedences.
This section describes how the training data for training a \emph{predicate} precedence recommender was generated.
Training data for training a function precedence recommender was generated analogously.

\subsubsection{Problem set}
\todo[inline]{MS: mozna bych dal propagoval cinny rod a minuly cas ...}
The examples are generated by sampling the \gls{fol} part of the problem library \gls{tptp} v7.4.0 \cite{10.1007/978-3-030-29436-6_29}.
A total of \num{17053} problems formulated in \gls{fof} and \gls{cnf} are sampled.
The problems formulated in \gls{fof} are converted to \gls{cnf} by \Vampire{}.
% vampire --mode clausify
Let $\ProblemsTptp$ denote the set of all \num{17053} \gls{cnf} problems available for training and evaluation.

\subsubsection{Sampling}

The examples are generated by iterative sampling of $\ProblemsTptp$.
In each iteration, a problem $P \in \ProblemsTptp$ is chosen and \Vampire{} is executed twice on $P$
with two (uniformly) random predicate precedences and a common random function precedence.
The ``background'' random function precedence serves as additional noise (in addition to the variability 
contained in \gls{tptp}) and makes sure that the predicate precedence recommender
will not be able to rely on any specificities that would come from fixing function precedences in the training data.

The two executions are compared in terms of performance:
predicate precedence $\PrecBetter$ is recognized as better than predicate precedence $\PrecWorse$,
denoted as $\Better{\PrecBetter}{\PrecWorse}{P}$,
if the proof search finishes successfully with $\PrecBetter$ ($\SolverRun{P}{\Prec} = \top$)
\todo{FB: Alias: We use $S$ for both solver and score of precedence pair.}
and if the number of iterations of the saturation loop with $\PrecBetter$ is smaller than with $\PrecWorse$.
\todo{FB: Use notation for saturation loop iterations if the notation was established earlier. 
If failure is denoted by $\infty$, we can simply use $<$.}
If one of the two precedences is recognized as better,
the example $(P, \PrecBetter, \PrecWorse)$ is produced,
where $\PrecBetter$ is the better precedence,
and $\PrecWorse$ is the other precedence.
Otherwise, for example, if \Vampire{} fails on both precedences, we go back to sampling another problem.
\todo{FB: Discuss: How come we mix examples in which both precedences terminate with examples in which only one precedence terminates?}

To ensure the efficiency of the sampling, the process is interpreted as an instance of Bernoulli multi-armed bandit problem,
\todo{Cite? MS: kdyz bude dobra citace, sem by se urcite sikla:) Pri nejhorsim Sutton \& Barto to jisti.}
with the reward of a trial being 1 in case an example is produced, and 0 otherwise.
Adaptive sampling balances
exploring problems that have been tried relatively scarcely and
exploiting problems that have yielded examples relatively often.
For each problem $P \in \ProblemsTptp$,
the generator keeps track of the number of times the problem has been tried $n_P$
and the number of examples generated from that problem $s_P$.
The ratio of $s_P$ to $n_P$ corresponds to the average reward of problem $P$ observed so far.
The problems are sampled using the allocation strategy \acrshort{ucb1} \cite{Auer2002} with a parallelizing relaxation.
In each iteration, the generator samples the problem $P$ that maximizes
$$
\frac{s_P}{n_P} + \sqrt{\frac{2 \ln n}{n_P}}
$$
\todo{FB: Shrink into text to save space.}
where $n = \sum_{P \in \ProblemsTptp} n_P$ is the total number of tries on all problems.
The parallelizing relaxation means that the $s_P$ values are only updated once in \num{1000} iterations,
allowing up to \num{2000} parallel solver executions.
\todo{Explain in detail?}

The sampling runs until a target number of examples is generated---\num{1000000} predicate precedence examples and \num{800000} function precedence examples.
% sftp://cluster.ciirc.cvut.cz/home/bartefil/git/vampire-ml/out/20210108-generate-predicate/questions_generated
For example, while generating \num{1000000} examples for the predicate precedence dataset,
the least explored problem was tried 19 times, and the most exploited problem was tried 504 times.
\num{5349} out of the \num{17053} problems yielded at least one example.

\subsubsection{Split}

The \num{17053} problems in $\ProblemsTptp$ are first split roughly in half to form the training set and the validation set.
Next, both training and validation sets are restricted
to problems whose graph representation consists of at most \num{100000} nodes
to limit the memory requirements of the training.
Approximately \SI{90}{\percent} of the problems fit into this limit
%a total of \num{15282} out of the \num{17053} (approximately \SI{90}{\percent}) problems satisfy this requirement.
and there are \num{7648} problems in the resulting validation set $\ProblemsVal$.
The training set $\ProblemsTrain$ is further restricted to problems that correspond to at least one training example,
leaving \num{2571} problems when training predicate precedence recommender,
and \num{1953} problems when training function precedence recommender.
\todo{FB: Unify the terminology: validation, or train set?
MS: Co by znamenalo unify? Nebo myslis test set?}
% /home/filip/projects/vampire-ml/vampire-ml/outputs/2021-02-11/16-27-02/results.csv
% all: 17053
% train: 8527
% val: 8526
% with_questions: 5349
% graphified: 15282
% train&with_questions: 2647
% train&with_questions&graphified: 2571
% val&graphified: 7648

\subsection{Hyperparameters}

We used a \gls{gcn} described in \cref{sec:gcn}
with depth 4, message size 16, \gls{relu} activation function,
skip connections \cite{Zhou2018}, and layer normalization \cite{Ba2016}.
We tuned the hyperparameters by a small manual grid search.
\todo[inline]{MS: zase pasivum. Tady to urcite bude chtit odkaz na sekci, kde se jasne definuje, co to znamena. Jinak recenzenti byvaji dost alergicti na hyperpametry, ktere se jen tak nahodi na nevysvetli. Minimalne nejaky komentar typu: treba, zkouseli jsme u ruzne dalsi hodnoty, ale dopadalo to podobne. Nebo, pozdeji rekeneme, co se deje, kdyz se tohle meni. Nebo: nemeli jsme cas zjistovat, co se deje, kdyz se to meni, tak berem rozumne hodnoty podle blabla.}

\subsection{Training procedure}

A predicate symbol cost model is trained by gradient descent
on the precedence pair classification task (described in \cref{sec:training})
using the examples generated from the problems in $\ProblemsTrain$.
To avoid redundant computations, all examples generated from a problem are processed in the same training batch.
Thus, each training batch contains up to \num{128} problems and all the examples generated from these problems.
The symbol cost model is trained using the Adam optimizer \cite{Kingma2014}.
Learning rate starts at \num{1.28e-3}
and is halved each time the train loss stagnates for 10 consecutive epochs.
% tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=10)

\subsubsection{Sample weighting}
Each of the training examples of problem $P$ contributes to the training with the weight $\frac{1}{s_P}$,
where $s_P$ is the number of examples of problem $P$ in the training set,
so that each problem contributes to the training to the same degree irrespective of the relative numbers of examples.

\iftoggle{LONG}{
\subsubsection{Metrics}
To analyze the training process,
we tracked the dynamics of three metrics on $\ProblemsTrain$ and $\ProblemsVal$:
loss, accuracy and solver performance.
The solver performance was estimated once per 10 training epochs by running \Vampire{}
on \num{1000} problems of the respective problem set
and counting the number of problems successfully solved within the time limit.
Since the accuracy is a proxy measure of solver performance and the loss is a proxy measure of the accuracy,
we are interested in generalization from loss to accuracy and from both loss and accuracy to solver performance.
Similarly, we are interested in generalization from training set to validation set.
\todo{MS: kdyz se to tady tak pekne pripravi, asi ctenar ceka, ze se to pozdeji 
nejak zuzitkuje. Pokud se rozhnodnem loss/accuracy/ATP zavislost dal nerozebirat,
asi je tu navic i tento odstavec, nebo aspon jeho cast.}
}

\subsubsection{Termination}
We run the training until the validation accuracy stopped increasing for 100 consecutive epochs.

\subsection{Final evaluation}

After the training finished,
we performed a final evaluation of the most promising intermediate trained model on the whole $\ProblemsVal$.
The model that manifested the best estimated solver performance on a sample of \num{1000} validation problems
was chosen as the most promising.
\todo[inline]{FB: Watch out: This way we may overfit on the 1000 problems used for sample solver performance evaluation. We should leave out these 1000 problems from the final evaluation.}
\todo[inline]{MS: taky me to ted napadlo! Uplne oficialni metodika byva split na train,valid,test. 
Kdy se trenuje na train, hyperparametry (vcetne early stopping) se ladi vuci valid 
a pak se vyhodnocuje na test. Nekdy se pak valid rika devel a test se rika holdout.}

\subsection{Results}

A predicate precedence recommender was trained on approximately \num{500000} examples,
% Half of the 1M predicate examples was used for validation.
and a function precedence recommender was trained on approximately \num{400000} examples.
% Half of the 800k function examples was used for validation.
For each problem $P \in \ProblemsVal$,
a predicate and a function precedence
was generated by the respective trained recommender,
and \Vampire{} was run using these precedences
with a wall clock time limit of 10 seconds.
% Note: We have mentioned the time limit above and here we repeat it for readers who only read Results.
The results are averaged over 5 runs to reduce the effect of noise due to the wall clock time limit.
As a baseline, the performance of \Vampire{} with the \texttt{frequency} precedence heuristic was evaluated
with the same time limit.
For comparison, the two trained recommenders are evaluated separately,
with the predicate precedence recommender using the \texttt{frequency} heuristic to generate the function precedences, and vice versa.

To generate a precedence for a problem,
\todo{MS: tenhle odstavec mozna prehodit s predchozim?}
the recommender first converts the problem to a machine-friendly \gls{cnf} format,
then converts the \gls{cnf} to a graph,
then predicts symbol costs using the \gls{gcn} model
and finally orders the symbols by their costs to produce the precedence.
To simplify the experiment, the time limit of 10 seconds is only imposed on the \Vampire{} run
and excludes the time taken by the recommender to generate the precedence.
When run with 2 threads,
the preprocessing of a single problem
takes at most at most \num[round-mode=places,round-precision=2]{1.262744486} seconds
for \SI{80}{\percent} of the problems
by extrapolation from a sample of \num{1000} problems.
% Analysis: https://docs.google.com/spreadsheets/d/1GujYNEtETpC3jk4iyENLjptv8mDmI5f1ZEhbmwtqnPM/edit?usp=sharing
% VML-715
\todo{MS: 1) single-core bych chapal jako default. tj netreba zminovat?
2) kterych 1000? To opet drazdi k otazkam. tj to bych asi taky vynechal, ale...
3) to maximum prece zalezi na tom, kterych 1000 to bylo.}
\todo{MS: Idea: kdyz je to linearni: co cas potrebny na recommending
vztahnout k casu klauzifikace? Jako rict, ze to trva x-krat dyl nez jen klauzifikovat?}
\Cref{tab:results} shows the results.

\begin{table*}[h]
\caption{
Results of the evaluation of symbol precedence heuristics based on various symbol cost models
on $\ProblemsVal$ ($\card{\ProblemsVal} = 7648$).
Means and standard deviations over 5 runs are reported.
The \gls{gcn} models are trained according to the description in \cref{sec:architecture,sec:evaluation}.
The simple model is the final model from \cite{DBLP:conf/cade/Bartek020}.
The models that use machine learning for the predicate precedence only
use the \texttt{frequency} heuristic for the function precedence, and vice versa.
The frequency models use the standard \texttt{frequency} heuristic implemented in \Vampire{}.
}
\label{tab:results}
\centering
\begin{tabular}{l|ll|rl}

Symbol cost model & \multicolumn{2}{l}{Successes on $\ProblemsVal$} & \multicolumn{2}{l}{Improvement over baseline} \\
& Mean & Std & Absolute & Relative \\

\hline


\acrshort{gcn} (predicate and function) &
% VML-706
\num{3951.6} &
\num[round-mode=places,round-precision=2]{1.624807680927192} &
%\SI{51.69}{\percent} &
+182.0 &
\num[round-mode=places,round-precision=3]{1.048280985} \\


\acrshort{gcn} (predicate only) &
% Success rate: mean: 0.513023013, std: 0.000292887
% Evaluation results: https://ui.neptune.ai/filipbartek/vampire-ml/e/VML-553
% Total: validation_solver_eval/all/problems/measured&split&category: 7648
% Difference in success count from baseline: 154 ~ 0.020135983
% Estimated difference from baseline (estimate on 891 problems): 0.021099888
% Checkpoint: outputs/2021-02-06/14-55-41/tf_ckpts/epoch/weights.00079-0.61.tf VML-540 0.511785
\num{3923.6} &
% Success mean: validation_solver_eval/all/success/count/mean: 3923.6
\num{2.24} &
% Success std: validation_solver_eval/all/success/count/std 2.24
%\SI{51.30}{\percent} &
% Success rate: 0.513023013
+154.0 &
\num[round-mode=places,round-precision=3]{1.040853141} \\


\acrshort{gcn} (function only) &
% Final evaluation: VML-677
% Evaluated checkpoint: outputs/2021-02-16/12-28-14/tf_ckpts/epoch/weights.00289.tf
% Results file: sftp://cluster.ciirc.cvut.cz/home/bartefil/git/vampire-ml/outputs/2021-02-17/12-01-09/solver_eval/symbol_cost/epoch_-1/logs.yaml
% Total: val/all/problems/measured&split&category: 7648
\num{3874.2} &
% Success mean: val/all/success/count/mean: 3874.2
\num[round-mode=places,round-precision=2]{1.8330302779823362} &
% Success std: val/all/success/count/mean: 1.8330302779823362
%\SI{50.66}{\percent} &
% Success rate: val/all/success/count/mean: 0.5065638075313807
+104.6 &
\num[round-mode=places,round-precision=3]{1.027748302} \\


Simple (predicate only) &
\num{3827.2} &
\num[round-mode=places,round-precision=2]{1.9390719429665317} &
%\SI{50.04}{\percent} &
+57.6 &
\num[round-mode=places,round-precision=3]{1.015280136} \\
% Final vector from PAAR paper: [0,0.429306921481749,0.57069307851825,0,0,0,0,0,0,0,0,0]
% Source: https://docs.google.com/spreadsheets/d/1HSsC7piUAtWt6uwA9SYOX5vmiF0Ab3Ae4FPyGsDALIg/edit#gid=99404775


%Frequency (regular \acrshort{kbo}) &
%\texttt{vampire -lcm standard} &
%\num{3823.0} &
%\num[round-mode=places,round-precision=2]{3.40587727318528} &
%\SI[round-mode=places,round-precision=2]{49.9869247}{\percent} &
%+53.4 &
%\num[round-mode=places,round-precision=3]{1.014165959} \\
% VML-741


Frequency &
%\texttt{vampire -lcm predicate} &
% Success rate: mean: 0.492887029, std: 0.000401412
% https://ui.neptune.ai/filipbartek/vampire-ml/e/VML-490
% /home/filip/projects/vampire-ml/vampire-ml/outputs/2021-02-09/12-13-43
% Row: 'val&graphified&solver_eval'
% Problems total: 7648
% Success rate: 0.492887029
\num{3769.6} &
\num{3.07} &
%\SI{49.29}{\percent} &
0.0 &
\num[round-mode=places,round-precision=3]{1.0} \\

\end{tabular}
\end{table*}
\todo{FB: Add links to Neptune runs?}

The results show that the \gls{gcn}-based model outperforms the \texttt{frequency} heuristic by a significant margin.
Since the predicate precedence recommender was trained against randomly distributed function precedences,
it is expected to perform well irrespective of the function precedence heuristic it is combined with.
Combining the trained recommenders for predicate and function precedences manifests better performance
than any of the two in combination with the standard \texttt{frequency} heuristic,
outperforming the \texttt{frequency} heuristic by approximately \SI{4.8}{\percent}.

We have verified that the simple linear predicate precedence heuristic
trained in \cite{DBLP:conf/cade/Bartek020} outperforms the \texttt{frequency} heuristic.
Moreover, we have confirmed the conjecture that using a \gls{gnn} may yield an even better performance \cite{DBLP:conf/cade/Bartek020}.

\iftoggle{LONG}{
In the process of training,
we tracked three metrics on the training and validation sets:
loss, accuracy, and solver performance on a sample of 1000 problems.
A typical training ran for approximately 500 epochs
with training loss steadily decreasing and
validation loss decreasing for the initial 50 epochs and then increasing.
Both training and validation accuracy were steadily increasing (training accuracy significantly faster than validation accuracy),
with validation loss entering a plateau around epoch 100.
The solver performance approximately followed the dynamics of the accuracy on the respective problem set.
% Predicate: https://ui.neptune.ai/filipbartek/vampire-ml/e/VML-540/charts
% Function: https://ui.neptune.ai/filipbartek/vampire-ml/e/VML-672/charts
}

\todo[inline]{FB: Discuss the phenomenon of increasing validation loss and accuracy.}
%A possible explanation of the discrepancy between the improving validation accuracy and deteriorating validation loss
%in the latter part of the training process
%is that the improvements of training loss generalize to improvements of loss on some, but not all of the validation examples.
%
%is that due to the limited generalization from training to validation set,
%some improvements of training loss come at the cost of an increase of loss on validation examples that are already misclassified
%(assigned a negative score).
%Such change increases the validation loss but does not affect the validation accuracy.
%At the same time there are validation examples whose loss is decreasing,
