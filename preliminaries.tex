% !TEX root = main.tex

\subsection{Saturation-based theorem proving}
\label{sec:saturation}

A \emph{\acrfull{fol} problem} consists of a set of axiom formulas and a conjecture formula.
\todo{FB: Clarify that we use FOL with equality.}
In a \emph{refutation-based} \emph{\acrfull{atp}},
proving that the axioms entail the conjecture
is reduced to proving that the axioms together with the negated conjecture entail a \emph{contradiction}.
The most popular \gls{fol} \glspl{atp}, such as \Vampire{} \cite{DBLP:conf/cav/KovacsV13}, E \cite{Schulz2019}, or SPASS \cite{DBLP:conf/cade/WeidenbachDFKSW09},
start the proof search by converting the input \gls{fol} formulas to an equisatisfiable representation in 
% \emph{quantifier-free first-order 
\emph{\acrfull{cnf}} \cite{DBLP:books/el/RV01/NonnengartW01,Harrison2009}.
We denote the problem in \gls{cnf} as $P = (\Sigma, \mathit{Cl})$,
% Handbook of AR, Chapter 3: signature $\Sigma = (P_\Sigma, F_\Sigma)$
\todo{FB: What about the variables? Handbook of AR (chapter 3) defines the set Var.}
where $\Sigma$ is a list of all non-logical (predicate and function)
\emph{symbols} in the problem called the \emph{signature},
and $\mathit{Cl}$ is the set of clauses of the problem (including the negated conjecture).

Given a problem $P$ in \gls{cnf},
a \emph{saturation-based} \gls{atp} searches for a refutational proof
by iteratively applying the \emph{inference rules} from the given \emph{calculus}
to infer new clauses entailed by $\mathit{Cl}$.
As soon as the empty clause, denoted by $\square$, is inferred,
the prover concludes that the premises entail the conjecture.
The sequence of those inferences leading up from the input clauses $\mathit{Cl}$ to the discovered $\square$ constitutes a proof.
If the premises do not entail the conjecture,
the proof search continues until
the set of inferred clauses is saturated with respect to the inference rules.
In the standard setting of time-restricted proof search, a time limit may end the process prematurely.

Since the space of derivable clauses is typically very large,
the efficacy of the prover depends on the order in which the inferences are applied.
The standard saturation-based \glspl{atp} order the inferences
by maintaining two classes of inferred clauses: processed and unprocessed \cite{Schulz2019}.
In each \emph{iteration of the saturation loop}, one clause (so-called \emph{given clause})
is selected from the unprocessed set
and all the inferences between the given clause and the processed clauses are performed.
The resulting new clauses are added to the unprocessed set.
Finishing the proof in few iterations of the saturation loop is important
because the number of inferred clauses typically grows exponentially during the proof search.

\todo{MS: Celkove hezky! Jen mi tu nejak chybi konec... A taky proc se to vsechno rika.
V PAAR clanku, pokud si dobre vzpominam, bylo hlavnim cilem zavest znaceni pro
 ``pocet iteraci nez se najde dukaz''. aby se pak pouzilo dale\dots
Nicmene ctu ted beginning->end, tak nevim, co chystas dale.\\
FB: Doplnil jsem motivaci do prvni vety odstavce.}

\todo[inline,author=FB]{Add terminology: $\bot$.}

\subsection{Superposition calculus}

The \emph{superposition calculus} is of particular interest
because it is used in the most successful contemporary \gls{fol} \glspl{atp}.
The inferences of the superposition calculus are constrained by a \emph{\gls{sot}} \cite{DBLP:journals/logcom/BachmairG94}.

The \gls{sot} influences the superposition calculus in two ways.
First, the inferences on each clause are limited
to the selected literals.
\todo{Cite Bachmair Ganzinger or Handbook of AR, chapter 2. For literal selection. Inspiration: Selecting the selection.}
\todo{MS: the original paper is \cite{DBLP:journals/logcom/BachmairG94}. 
Stoji za precteni, at se v tom trochu vyznas!
Mimochodem, mozna ze ``simplification ordering'' neni ten uplne nejpresnejsi nazev. 
Pripadne na ten clenek mrkni rychle uz ted. V tehle sekci by se hodily nejake citace,
aby bylo jasne, od koho terminy prejimame.
(Weidenbachova kapitola v Handbooku taku (Combining superposition, sorts and splitting)
taky tusim definuje presne vlastnosti toho usporadani a dava jim jmeno.}
In each clause,
either a negative literal or all maximal literals are selected.
The maximality is evaluated
according to the simplification ordering.
% Note: The selection function Total does not use the simplification ordering.
\todo{MS: Ta logika je (pro pripomenuti):
1) pravidla jsou sound i bez restrikci, (proto selection Total je v pohode)
2) nicmene se ukazuje, ze cim mene toho selectem a tedy cim mene bude provedeno inferenci, tim lepe typicky pro search (vetsi sance dokazat (i tezke) problemy)
3) na treti stranu, nemuzem nedelat nic - to by nebylo complete.
4) Pravidla o selekci literalu, ktera popisujes, vyjadruji
minimalni selekci, o ktere se podarilo dokazat (BachmairGanzinger), ze zaruci uplnost. 
}
Second, the simplification ordering orients some of the equalities
to prevent superposition and equality factoring
from inferring redundant complex conclusions.
In each of these two roles,
the simplification ordering may impact the direction and,
in effect, the length of the proof search.

The \emph{\acrfull{kbo}} \cite{Knuth1983}, a commonly used simplification ordering scheme,
is parameterized by symbol weights and a \emph{symbol precedence},
a permutation\footnote{
The definition of KBO does not require the precedence to be total. 
However, for use in ATPs, the more symbols and thus also terms 
we can compare, the better. % That is why we define a precedence as a permutation.
} of the non-logical symbols of the input problem.
In this work, we focus on the task of finding a symbol precedence which
leads to a good performance of an ATP
when plugged into the \gls{kbo},
leaving all the symbol weights at the default 
value 1 as set by the \gls{atp} \Vampire{}.

\subsection{\Acrlongpl{nn}}

A \emph{feedforward \acrlong{ann}} \cite{DBLP:books/daglib/0040158} is a \acrlong{dag} of \emph{modules}.
Each module is an operation that consumes a numeric (input) \emph{vector} and outputs a numeric vector.
Each of the components of the output vector is called a \emph{unit} of the module.
The output of each module is differentiable with respect to the input almost everywhere.
\todo{FB: Should we explain why "almost everywhere" suffices?}

The standard modules include
the \emph{fully connected layer}, which performs an affine transformation,
and non-linear \emph{activation functions} such as the \emph{\gls{relu}} or \emph{sigmoid}.\footnote{
These are, respectively, $f(x) = \max\{0,x\}$ and $g(x) = \frac{1}{1+e^{-x}}$.}
A fully connected layer with a single unit is called the \emph{linear unit}.

Some of the modules are parameterized by numeric \emph{parameters}.
For example, the fully connected layer that transforms the input $x$ by the affine transformation $Wx + b$
is parameterized by the weight matrix $W$ and the bias vector $b$.
If the output of a module is differentiable with respect to a parameter,
that parameter is considered \emph{trainable}.

In a typical scenario, the \acrlong{nn} is trained by \emph{gradient descent} on a \emph{training set} of \emph{examples}.
% Note: In case of GCN, the example is actually a set of edge matrices.
In such a setting, the network outputs a single numeric value called \emph{loss} when evaluated on a \emph{batch} of examples.
The loss of a batch is typically computed as a weighted sum of the losses of the individual examples.
Since each of the modules is differentiable with respect to its input and trainable parameters,
the gradient of the loss with respect to all trainable parameters of the neural network
can be computed using the \emph{back-propagation} algorithm \cite{DBLP:books/daglib/0040158}.
The trainable parameters are then updated by taking a small step
against the gradient---in the direction that is expected to reduce the loss.
An \emph{epoch} is a sequence of iterations that updates the trainable parameters
using each example in the training set exactly once.

A \emph{\gls{gcn}} is a special case of feedforward \acrlong{nn}.
The modules of a \gls{gcn} transform messages that are passed along the edges of a graph encoded in the input example.
A particular architecture of a \gls{gcn} used prominently in this work is discussed in \cref{sec:gcn}.
