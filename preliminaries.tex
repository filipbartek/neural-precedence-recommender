% !TEX root = main.tex

% Terminology

\todo[inline,author=FB]{Get inspiration from PAAR paper.}

\subsection{Saturation-based theorem proving}
\label{sec:saturation}

A \emph{\acrfull{fol} problem} consists of a set of axiom formulas and a conjecture formula.
In a \emph{refutation-based} \emph{\acrfull{atp}},
proving that the axioms entail the conjecture
is reduced to proving that the axioms together with the negated conjecture entail a \emph{contradiction}.
The most popular \gls{fol} \glspl{atp}, such as \Vampire{} \cite{10.1007/978-3-642-39799-8_1}, E \cite{10.1007/978-3-030-29436-6_29}, or SPASS \cite{DBLP:conf/cade/WeidenbachDFKSW09},
start the proof search by converting the input \gls{fol} formulas to an equisatisfiable representation in 
% \emph{quantifier-free first-order 
\emph{\acrfull{cnf}} \cite{DBLP:books/el/RV01/NonnengartW01,Harrison2009}.
We denote a problem in \gls{cnf} as $P = (\Sigma, \mathit{Cl})$,
where $\Sigma$ is a list of all non-logical (predicate and function)
\emph{symbols} in the problem called the \emph{signature},
and $\mathit{Cl}$ is the set of the clauses of the problem (including the negated conjecture).

Given a problem $P$ in \gls{cnf},
a \emph{saturation-based} \gls{atp} searches for a refutational proof
by iteratively applying the \emph{inference rules} from the given \emph{calculus}
to infer new clauses entailed by $\mathit{Cl}$.
As soon as the empty clause, denoted by $\square$, is inferred,
the prover concludes that the premises entail the conjecture,
the sequence of inferences taken constituting a proof.
In case the premises do not entail the conjecture,
the proof search continues until
the set of the inferred clauses is saturated with respect to the inference rules.
In the common setting of time-restricted proof search, a time limit may end the process prematurely.

The common saturation-based \glspl{atp} structure the proof search
by maintaining two classes of the inferred clauses: processed and unprocessed \cite{10.1007/978-3-030-29436-6_29}.
In each \emph{iteration of the saturation loop}, one clause (so-called \emph{given clause})
is selected from the unprocessed set
and all the inferences between the given clause and the processed clauses are performed.
The resulting new clauses are added to the unprocessed set.

\todo[inline,author=FB]{Add terminology: $\bot$.}

\subsection{Superposition calculus}

The superposition calculus is of special interest
because it is used in the most successful \gls{fol} \glspl{atp}.

The inference rules of superposition calculus are constrained by a \gls{sot}.

\todo[inline,author=FB]{Mention: Vampire, superposition calculus, inference rules (resolution, superposition), simplification ordering on terms, KBO, symbol precedence. How does the STO influence the rules? Mention transfinite KBO (lcm predicate), which is only relevant to literal selection. Cite Voronkov: transfinite KBO (see section Notes on implementation). We use TKBO to increase the impact of predicate precedence. Vampire sets all KBO weights to 1.}

\subsubsection{\Gls{sot}}

An ordering on terms is called a \emph{simplification ordering} if it satisfies the following conditions:

...

influences the proof search in \Vampire{} on two levels.
First, the inferences on each clause are limited
to the selected literals.
\todo{Cite Bachmair Ganzinger or Handbook of AR, chapter 2. For literal selection. Inspiration: Selecting the selection.}
In each clause,
either a negative literal or all the maximal literals are selected.
The maximality is evaluated
according to the simplification ordering.
% Note: The selection function Total does not use the simplification ordering.
Second, the simplification ordering orients some of the equalities
to prevent superposition and equality factoring
from inferring redundant complex conclusions.
In each of these two roles,
the simplification ordering may impact the direction and,
in effect, the length of the proof search.

\subsubsection{\Acrfull{kbo}}

% Example: It seems that a classical example for KBO an orienting equations is group theory axioms.




Recall the two views:
\begin{enumerate}
\item
	One is the view of complete literal selections (we influence which literals are maximal)

\item
	The other is that of the term rewriting world

\end{enumerate}
(The full landscape also contains LPO).




\subsubsection{Symbol precedences}

\todo[inline,author=FB]{Define terminology.}

\subsection{\Acrlongpl{nn}}

A \emph{feedforward \acrlong{ann}} \cite{DBLP:books/daglib/0040158} is a \acrlong{dag} of \emph{modules}.
Each module is an operation that consumes a numeric (input) \emph{vector} and outputs a numeric vector.
Each of the components of the output vector is called a \emph{unit} of the module.
The output of each module is differentiable with respect to the input.

The common modules include
the \emph{fully connected layer}, which performs an affine transformation,
and non-linear \emph{activation functions} such as the \emph{\gls{relu}} or \emph{sigmoid}.\footnote{
These are, respectively, $f(x) = \max\{0,x\}$ and $g(x) = \frac{1}{1+e^{-x}}$.}
A fully connected layer with a single unit is called the \emph{linear unit}.

Some of the modules are parameterized by numeric \emph{parameters}.
For example, the fully connected layer that transforms the input $x$ by the affine transformation $Wx + b$
is parameterized by the weight matrix $W$ and the bias vector $b$.
If the output of a module is differentiable with respect to a parameter,
that parameter is considered \emph{trainable}.

In a typical use case, the \acrlong{nn} is trained by \emph{gradient descent} on a \emph{training set} of \emph{examples}.
% Note: In case of GCN, the example is actually a set of edge matrices.
In such setting, the network outputs a single numeric value called \emph{loss} when evaluated on a \emph{batch} of examples.
The loss of a batch is typically computed as a weighted sum of the losses of the individual examples.
Since each of the modules is differentiable with respect to its inputs and trainable parameters,
the gradient of the loss in the trainable parameters
\todo{FB: Is this terminology correct?}
can be computed using the chain rule \cite{DBLP:books/daglib/0040158}.
The trainable parameters are then updated by taking a small step
against the gradient---in the direction that is expected to make the loss value smaller.
An \emph{epoch} is a sequence of iterations that updates the trainable parameters
using each example in the training set exactly once.
