% !TEX root = main.tex

\subsection{Saturation-based theorem proving}
\label{sec:saturation}

A \emph{\acrfull{fol} problem} consists of a set of axiom formulas and a conjecture formula.
In a \emph{refutation-based} \emph{\acrfull{atp}},
proving that the axioms entail the conjecture
is reduced to proving that the axioms together with the negated conjecture entail a \emph{contradiction}.
The most popular \gls{fol} \glspl{atp}, such as \Vampire{} \cite{10.1007/978-3-642-39799-8_1}, E \cite{10.1007/978-3-030-29436-6_29}, or SPASS \cite{DBLP:conf/cade/WeidenbachDFKSW09},
start the proof search by converting the input \gls{fol} formulas to an equisatisfiable representation in 
% \emph{quantifier-free first-order 
\emph{\acrfull{cnf}} \cite{DBLP:books/el/RV01/NonnengartW01,Harrison2009}.
We denote a problem in \gls{cnf} as $P = (\Sigma, \mathit{Cl})$,
where $\Sigma$ is a list of all non-logical (predicate and function)
\emph{symbols} in the problem called the \emph{signature},
and $\mathit{Cl}$ is the set of the clauses of the problem (including the negated conjecture).

Given a problem $P$ in \gls{cnf},
a \emph{saturation-based} \gls{atp} searches for a refutational proof
by iteratively applying the \emph{inference rules} from the given \emph{calculus}
to infer new clauses entailed by $\mathit{Cl}$.
As soon as the empty clause, denoted by $\square$, is inferred,
the prover concludes that the premises entail the conjecture,
the sequence of inferences taken constituting a proof.
In case the premises do not entail the conjecture,
the proof search continues until
the set of the inferred clauses is saturated with respect to the inference rules.
In the common setting of time-restricted proof search, a time limit may end the process prematurely.

The common saturation-based \glspl{atp} structure the proof search
by maintaining two classes of the inferred clauses: processed and unprocessed \cite{10.1007/978-3-030-29436-6_29}.
In each \emph{iteration of the saturation loop}, one clause (so-called \emph{given clause})
is selected from the unprocessed set
and all the inferences between the given clause and the processed clauses are performed.
The resulting new clauses are added to the unprocessed set.

\todo{MS: Celkove hezky! Jen mi tu nejak chybi konec... A taky proc se to vsechno rika.
V PAAR clanku, pokud si dobre vzpominam, bylo hlavnim cilem zavest znaceni pro
 ``pocet iteraci nez se najde dukaz''. aby se pak pouzilo dale\dots
Nicmene ctu ted beginning->end, tak nevim, co chystas dale. }

\todo[inline,author=FB]{Add terminology: $\bot$.}

\subsection{Superposition calculus}

The \emph{superposition calculus} is of special interest
because it is used in the most successful contemporary \gls{fol} \glspl{atp}.
The inferences of superposition calculus are constrained by a \emph{\gls{sot}}.

The \gls{sot} influences the superposition calculus in two ways.
First, the inferences on each clause are limited
to the selected literals.
\todo{Cite Bachmair Ganzinger or Handbook of AR, chapter 2. For literal selection. Inspiration: Selecting the selection.}
\todo{MS: the original paper is \cite{DBLP:journals/logcom/BachmairG94}. 
Stoji za precteni, at se v tom trochu vyznas!
Mimochodem, mozna ze ``simplification ordering'' neni ten uplne nejpresnejsi nazev. 
Pripadne na ten clenek mrkni rychle uz ted. V tehle sekci by se hodily nejake citace,
aby bylo jasne, od koho terminy prejimame.
(Weidenbachova kapitola v Handbooku taku (Combining superposition, sorts and splitting)
taky tusim definuje presne vlastnosti toho usporadani a dava jim jmeno.}
In each clause,
either a negative literal or all the maximal literals are selected.
The maximality is evaluated
according to the simplification ordering.
% Note: The selection function Total does not use the simplification ordering.
\todo{MS: Ta logika je (pro pripomenuti):
1) pravidla jsou sound i bez restrikci, (proto selection Total je v pohode)
2) nicmene se ukazuje, ze cim mene toho selectem a tedy cim mene bude provedeno inferenci, tim lepe typicky pro search (vetsi sance dokazat (i tezke) problemy)
3) na treti stranu, nemuzem nedelat nic - to by nebylo complete.
4) Pravidla o selekci literalu, ktera popisujes, vyjadruji
minimalni selekci, o ktere se podarilo dokazat (BachmairGanzinger), ze zaruci uplnost. 
}
Second, the simplification ordering orients some of the equalities
to prevent superposition and equality factoring
from inferring redundant complex conclusions.
In each of these two roles,
the simplification ordering may impact the direction and,
in effect, the length of the proof search.

The \emph{\acrfull{kbo}}, a commonly used simplification ordering scheme,
is parameterized by symbol weights and a \emph{symbol precedence},
a permutation of the non-logical symbols of the input problem.
In this work we focus on finding a symbol precedence which 
leads to a good performance of an ATP
when plugged into the \gls{kbo},
leaving all the symbol weights at the default 
value 1 as set by the \gls{atp} \Vampire{}.

\subsection{\Acrlongpl{nn}}

A \emph{feedforward \acrlong{ann}} \cite{DBLP:books/daglib/0040158} is a \acrlong{dag} of \emph{modules}.
Each module is an operation that consumes a numeric (input) \emph{vector} and outputs a numeric vector.
Each of the components of the output vector is called a \emph{unit} of the module.
The output of each module is differentiable with respect to the input.
\todo{MS: mozna bych byl opetrnejsi. Co ReLU a derivace v bode zlomu? ;)}

The common modules include
the \emph{fully connected layer}, which performs an affine transformation,
and non-linear \emph{activation functions} such as the \emph{\gls{relu}} or \emph{sigmoid}.\footnote{
These are, respectively, $f(x) = \max\{0,x\}$ and $g(x) = \frac{1}{1+e^{-x}}$.}
A fully connected layer with a single unit is called the \emph{linear unit}.

Some of the modules are parameterized by numeric \emph{parameters}.
For example, the fully connected layer that transforms the input $x$ by the affine transformation $Wx + b$
is parameterized by the weight matrix $W$ and the bias vector $b$.
If the output of a module is differentiable with respect to a parameter,
that parameter is considered \emph{trainable}.

In a typical use case, the \acrlong{nn} is trained by \emph{gradient descent} on a \emph{training set} of \emph{examples}.
% Note: In case of GCN, the example is actually a set of edge matrices.
In such setting, the network outputs a single numeric value called \emph{loss} when evaluated on a \emph{batch} of examples.
The loss of a batch is typically computed as a weighted sum of the losses of the individual examples.
Since each of the modules is differentiable with respect to its inputs and trainable parameters,
the gradient of the loss in the trainable parameters
\todo{FB: Is this terminology correct?}
can be computed using the chain rule \cite{DBLP:books/daglib/0040158}.
The trainable parameters are then updated by taking a small step
against the gradient---in the direction that is expected to make the loss value smaller.
An \emph{epoch} is a sequence of iterations that updates the trainable parameters
using each example in the training set exactly once.

\todo{MS: Hezke! Ted uz v arichtekture muzes mavat pojmy o neuronkach a bude to tu uktovne! } 