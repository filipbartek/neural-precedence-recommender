\documentclass[runningheads]{llncs}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[binary-units=true,detect-weight=true]{siunitx}

% How to write a paper:
% https://mj.ucw.cz/papers/jakpsat.pdf
% Jones 2016: https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/
% AWR: https://jazyky.fel.cvut.cz/vyuka/RPP/BE9M04AKP/

% PAAR paper: https://github.com/filipbartek/learning-precedences-from-elementary-symbol-features/releases/download/paar2020%2Fceur-2/Learning_Precedences_from_Simple_Symbol_Features.pdf

\input{preamble}
\input{glossary}
\input{front}

\usepackage{svg}

\usepackage[pdf]{graphviz}
\usepackage{tikz}

\usepackage{hyperref}
\hypersetup{
pdftitle={Neural Precedence Recommender},
pdfauthor={Filip BÃ¡rtek, Martin Suda},
pdfkeywords={saturation-based theorem proving, simplification ordering, symbol precedence, machine learning, graph convolutional network}
}

\usepackage{cleveref}

% Some mild shrinkage:

% for Section and subsection headings:
%% Save the class definition of \subparagraph
\let\llncssubparagraph\subparagraph
%% Provide a definition to \subparagraph to keep titlesec happy
\let\subparagraph\paragraph
%% Load titlesec
\usepackage{titlesec}
%% Revert \subparagraph to the llncs definition
\let\subparagraph\llncssubparagraph
\titlespacing*{\subsection}
  {0pt}{0.9\baselineskip}{0.8\baselineskip}

% For figures
% https://tex.stackexchange.com/questions/60477/remove-space-after-figure-and-before-text
\setlength{\textfloatsep}{0.7\baselineskip plus 0.2\baselineskip minus 0.2\baselineskip} % between a float at the top and text
\setlength{\floatsep}{0.3\baselineskip plus 0.2\baselineskip minus 0.2\baselineskip} % between two floats at the top
\setlength{\intextsep}{0.7\baselineskip plus 0.2\baselineskip minus 0.2\baselineskip} % between a float in text and text

\begin{document}

\maketitle

\begin{abstract}
\input{abstract}
\end{abstract}

\section{Introduction}

\input{introduction}

\section{Preliminaries}
\label{sec:preliminaries}
\input{preliminaries}

\section{Architecture}
\label{sec:architecture}
\input{architecture}

\section{Experimental evaluation}
\label{sec:evaluation}
\input{evaluation}

\section{Related work}
\label{sec:related}

Our previous text \cite{DBLP:conf/cade/Bartek020} marked the initial investigation of applying techniques of \gls{ml}
to generating good symbol precedences.
The neural recommender presented here uses a \gls{gnn} to model symbol costs,
while \cite{DBLP:conf/cade/Bartek020} used a linear combination of symbol features readily available in the \gls{atp} \Vampire{}.
The \gls{gnn}-based approach yields more performant precedences at the cost of longer training and preprocessing time.

In \cite{Olsak2019} and \cite{Rawson2020}, the authors propose similar \gls{gnn} architectures to solve tasks on \gls{fol} problems.
They use the \glspl{gnn} to solve classification tasks such as premise selection.
While our system is trained on a proxy classification task,
the main task it is evaluated on is the generation of useful precedences.

\todo[inline]{Related problem: Ranking. See Mohri etc.}

\section{Conclusion}
\label{sec:conclusion}

We have described a system that extracts useful symbol precedences from the graph representations of \gls{cnf} problems.
Comparison against a conventional symbol precedence heuristic shows that using a \gls{gcn}
to consider the whole structure of the input problem is beneficial.
\todo{FB: Should we boast that the reduction of ranking to training of differentiable element scores is innovative?}

\subsection{Future work}

An analysis of the trained recommender could reveal new insights into how the symbol precedence influences the proof search,
and how a symbol precedence should be chosen in the context of a problem so that it is likely to lead to an efficient proof search.
This might lead to a design of a new precedence heuristic
that focuses relies on aspects of input problems
that have been overlooked so far in the context of generating symbol precedences.

While using the \gls{kbo} as our simplification ordering scheme of choice,
we restricted ourselves to generating the symbol precedences,
leaving the symbol weights fixed to the constant 1 (the default behavior of \Vampire{}).
Learning to recommend the symbol weights in addition to the symbol precedences would be interesting.
\todo{FB: Be more fancy in wording.}

Training both the predicate and the function precedence using a single \gls{gcn} would yield a recommender
that only needs one pass through a \gls{gcn} to generate both predicate and function precedences for a problem.
Moreover, the model would have the possibility to exploit the properties of the function precedence
when generating the predicate precedence, and vice versa.
Finally, higher training data efficiency could be achieved by considering all pairs of measured executions on a problem
in one training batch.
\todo{FB: Confusing?}

%A more detailed analysis of the training process might yield insights into how the performance generalizes
%from the training set to the validation set
%and from proxy metrics, such as the loss and the accuracy, to the final solver performance.
%Ultimately, this might reveal modifications that make the training more efficient and the final recommender more performant.
%
%The training examples take the form of pairs of precedences,
%where one of the precedences is preferable over the other.
%In this work, the amount by which the quality of the precedences differs is ignored.
%Taking the exact or estimated numbers of saturation loop iterations into account
%might help the recommender to prioritize the more significant gains.
%
%Applying techniques of reinforcement learning to focus the training data on the cases that yield high performance gains
%could further improve the recommender performance.
%
%We restricted our experiments to problems whose graph representation does not exceed \num{100000} nodes.
%Evaluating the recommender on larger problems would be an interesting test of generalization.
%Training on larger problems may be facilitated by employing a lossy graphification.
%
%We performed all experiments with a fixed \gls{atp} \Vampire{} and a fixed configuration
%we consider to be a reasonable baseline.
%Introducing other prover configurations (namely other \glspl{sot})
%or other provers could make the resulting recommender more robust
%and possibly help extract more general knowledge about the structure of good symbol precedences.
%
%Finally, the approach outlined in \cref{sec:architecture}
%could be adjusted to solve tasks on \gls{fol} formulas other than recommending symbol precedences,
%such as premise selection of given clause selection.
%On the other hand, we could try to solve other tasks that involve generating permutations of arbitrary length.
%\todo{FB: TSP? Probably not because it is too combinatorial.}

\section*{Acknowledgments}
% > Acknowledgements should generally be placed in an unnumbered subsection at the end of the paper.

% https://sgs.cvut.cz/index.php?action=faq
% This work was supported by the Grant Agency of the Czech Technical University in Prague, grant No. SGS...

\todo{MS: zvazit, jestli nerozepsat zvlast Martin, zvlast Filip.
Pripadne se zeptat Hanky, jestli bychom nemeli nejak vypichnout
jen GACR.}

This work was supported by
the Czech Science Foundation project 20-06390Y,
the project RICAIP no. 857306 under the EU-H2020 programme,
and
the Grant Agency of the Czech Technical University in Prague, grant\\
no.~SGS20/215/OHK3/3T/37.


% > LaTeX users should avoid self-defined environments and use the bibliographic style MathPhySci for computer science proceedings.
% > It is not possible to have hyperlinks in references.
\bibliographystyle{splncs04}
\bibliography{main}

%\appendix
%\input{appendix}

\end{document}
