Our previous text \cite{DBLP:conf/cade/Bartek020} marked the initial investigation of applying techniques of \acrlong{ml}
to generating good symbol precedences.
The neural recommender presented here uses a \gls{gnn} to model symbol costs,
while \cite{DBLP:conf/cade/Bartek020} used a linear combination of symbol features readily available in the \gls{atp} \Vampire{}.
The \gls{gnn}-based approach yields more performant precedences at the cost of longer training and preprocessing time.

In \cite{Olsak2019}, \cite{Jakubuv2020} and \cite{Rawson2020}, the authors propose similar \gls{gnn} architectures to solve tasks on \gls{fol} problems.
They use the \glspl{gnn} to solve classification tasks such as premise selection.
While our system is trained on a proxy classification task,
the main task it is evaluated on is the generation of useful precedences.
% Jakubuv2020 uses the same architecture as Olsak2019.
% Our GNN uses a different particular architecture than Olsak2019.

The problem of learning to rank objects represented by scores trainable by gradient descent was explored in \cite{Burges2005}.
Our work can be seen to apply the approach of \cite{Burges2005} to rank permutations represented by weighted sums of symbol costs.
\todo{Related problem: Ranking. See Mohri etc.}

% Fast differentiable ranking (and probably other texts): They use the permutation scoring by weighted sum.
