@string{lncs = "Lecture Notes in Computer Science"}

@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = 2,
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018}
}

@inproceedings{DBLP:conf/cade/Bartek020,
  author    = {Filip B{\'{a}}rtek and
               Martin Suda},
  title     = {Learning Precedences from Simple Symbol Features},
  pages     = {21--33},
  crossref  = {DBLP:conf/cade/2020paar},
  url       = {http://ceur-ws.org/Vol-2752/paper2.pdf},
  timestamp = {Tue, 09 Feb 2021 17:28:14 +0100},
  biburl    = {https://dblp.org/rec/conf/cade/Bartek020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@proceedings{DBLP:conf/cade/2020paar,
  editor    = {Pascal Fontaine and
               Konstantin Korovin and
               Ilias S. Kotsireas and
               Philipp R{\"{u}}mmer and
               Sophie Tourret},
  booktitle = {Practical Aspects of Automated Reasoning and Satisfiability Checking and Symbolic Computation Workshop},
  title     = {Joint Proceedings of the 7th Workshop on Practical Aspects of Automated
               Reasoning {(PAAR)} and the 5th Satisfiability Checking and Symbolic
               Computation Workshop (SC-Square) Workshop, 2020 co-located with the
               10th International Joint Conference on Automated Reasoning {(IJCAR}
               2020)},
  series    = {{CEUR} Workshop Proceedings},
  address   = {Aachen},
  issn      = {1613-0073},
  number    = 2752,
  publisher = {CEUR-WS.org},
  year      = 2020,
  url       = {http://ceur-ws.org/Vol-2752},
  venue     = {Paris, France},
  eventdate = {2020-06-29},
  urn       = {urn:nbn:de:0074-2752-0},
  timestamp = {Tue, 23 Feb 2021 01:16:56 +0100},
  biburl    = {https://dblp.org/rec/conf/cade/2020paar.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% http://www.eprover.org/EVENTS/PAAR-2020.html
% ceur-ws.org/HOWTOSUBMIT.html

% Argument-order-preserving graphification by the use of auxiliary argument nodes
% Section 5, paragraph Representation
@inproceedings{Rawson2020,
author = {Rawson, Michael and Reger, Giles},
pages = {109--119},
title = {Directed Graph Networks for Logical Reasoning (Extended Abstract)},
url = {http://ceur-ws.org/Vol-2752/paper8.pdf},
crossref = {DBLP:conf/cade/2020paar}
}

@inproceedings{RegerSuda2017,
  author       = {Giles Reger and Martin Suda},
  title        = {Measuring progress to predict success: Can a good proof strategy be evolved?},
  year         = {2017},
  booktitle    = {AITP 2017},
  pages        = {20--21},
  url          = {http://aitp-conference.org/2017/aitp17-proceedings.pdf},
}
% http://aitp-conference.org/2017/

@inproceedings{DBLP:conf/cade/HustadtKS05,
  author    = {Ullrich Hustadt and
               Boris Konev and
               Renate A. Schmidt},
  title     = {Deciding Monodic Fragments by Temporal Resolution},
  pages     = {204--218},
  crossref  = {DBLP:conf/cade/2005},
  doi       = {10.1007/11532231\_15},
  timestamp = {Tue, 14 May 2019 10:00:39 +0200},
  biburl    = {https://dblp.org/rec/conf/cade/HustadtKS05.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@proceedings{DBLP:conf/cade/2005,
  editor    = {Robert Nieuwenhuis},
  booktitle = {Automated Deduction -- CADE-20},
  venue     = {Tallinn, Estonia},
  eventdate = {July 22-27, 2005},
  series    = lncs,
  volume    = {3632},
  publisher = {Springer},
  address   = {Berlin, Heidelberg},
  year      = {2005},
  doi       = {10.1007/11532231},
  isbn      = {3-540-28005-7},
  timestamp = {Mon, 22 Feb 2021 21:51:16 +0100},
  biburl    = {https://dblp.org/rec/conf/cade/2005.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/lics/GanzingerN99,
  author    = {Harald Ganzinger and
               Hans de Nivelle},
  title     = {A Superposition Decision Procedure for the Guarded Fragment with Equality},
  pages     = {295--303},
  crossref  = {DBLP:conf/lics/1999},
  doi       = {10.1109/LICS.1999.782624},
  timestamp = {Wed, 16 Oct 2019 14:14:54 +0200},
  biburl    = {https://dblp.org/rec/conf/lics/GanzingerN99.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@proceedings{DBLP:conf/lics/1999,
  booktitle     = {14th Annual {IEEE} Symposium on Logic in Computer Science},
  venue     = {Trento, Italy},
  eventdate = {July 2-5, 1999},
  publisher = {{IEEE} Computer Society},
  year      = {1999},
  isbn      = {0-7695-0158-3},
  timestamp = {Mon, 22 Feb 2021 21:50:27 +0100},
  biburl    = {https://dblp.org/rec/conf/lics/1999.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% https://ieeexplore.ieee.org/xpl/conhome/6352/proceeding

@InProceedings{Reger2016,
  author    = {Giles Reger and Martin Suda and Andrei Voronkov},
  title     = {New Techniques in Clausal Form Generation},
  booktitle = {GCAI 2016. 2nd Global Conference on Artificial Intelligence},
  year      = {2016},
  editor    = {Christoph Benzm\"uller and Geoff Sutcliffe and Raul Rojas},
  volume    = {41},
  series    = {EPiC Series in Computing},
  pages     = {11--23},
  publisher = {EasyChair},
  bibsource = {EasyChair, https://easychair.org},
  doi       = {10.29007/dzfz},
  issn      = {2398-7340},
  owner     = {filip},
  timestamp = {2019-12-03},
}

@incollection{Bachmair89completionwithout,
title = {Completion Without Failure},
editor = {Hassan Aït-Kaci and Maurice Nivat},
booktitle = {Rewriting Techniques},
publisher = {Academic Press},
pages = {1--30},
year = {1989},
isbn = {978-0-12-046371-8},
doi = {10.1016/B978-0-12-046371-8.50007-9},
author = {Leo Bachmair and Nachum Derschowitz and David A. Plaisted}
}

@incollection{DBLP:books/el/RV01/BachmairG01,
  author    = {Leo Bachmair and
               Harald Ganzinger},
  title     = {Resolution Theorem Proving},
  pages     = {19--99},
  crossref  = {DBLP:books/el/RobinsonV01},
  doi       = {10.1016/b978-044450813-3/50004-7},
  timestamp = {Thu, 25 Jul 2019 12:26:00 +0200},
  biburl    = {https://dblp.org/rec/books/el/RV01/BachmairG01.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{Robinson1983,
author="Robinson, G.
and Wos, Larry",
editor="Siekmann, J{\"o}rg H.
and Wrightson, Graham",
title="Paramodulation and Theorem-Proving in First-Order Theories with Equality",
bookTitle="Automation of Reasoning: 2: Classical Papers on Computational Logic 1967--1970",
year="1983",
publisher="Springer",
address="Berlin, Heidelberg",
pages="298--313",
abstract="A term is an individual constant or variable or an n-adic function letter followed by n terms. An atomic formula is an n-adic predicate letter followed by n terms. A literal is an atomic formula or the negation thereof. A clause is a set of literals and is thought of as representing the universally-quantified disjunction of its members. It will sometimes be notationally convenient1 to distinguish between the empty clause □, viewed as a clause, and `other' empty sets such as the empty set of clauses, even though all these empty sets are the same set-theoretic object {\o}. A ground clause (term, literal) is one with no variables. A clause C' (literal, term) is an instance of another clause C (literal, term) if there is a uniform replacement of the variables in C by terms that transform C into C'.",
isbn="978-3-642-81955-1",
doi="10.1007/978-3-642-81955-1\_19",
}

@incollection{DBLP:books/el/RV01/NieuwenhuisR01,
  author    = {Robert Nieuwenhuis and
               Albert Rubio},
  title     = {Paramodulation-Based Theorem Proving},
  pages     = {371--443},
  crossref  = {DBLP:books/el/RobinsonV01},
  doi       = {10.1016/b978-044450813-3/50009-6},
  timestamp = {Thu, 25 Jul 2019 12:26:00 +0200},
  biburl    = {https://dblp.org/rec/books/el/RV01/NieuwenhuisR01.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Vampire
@inproceedings{DBLP:conf/cav/KovacsV13,
  author    = {Laura Kov{\'{a}}cs and
               Andrei Voronkov},
  title     = {First-Order Theorem Proving and {V}ampire},
  pages     = {1--35},
  crossref  = {DBLP:conf/cav/2013},
  doi       = {10.1007/978-3-642-39799-8\_1},
  timestamp = {Tue, 14 May 2019 10:00:43 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/cav/KovacsV13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@proceedings{DBLP:conf/cav/2013,
  editor    = {Natasha Sharygina and
               Helmut Veith},
  booktitle = {Computer Aided Verification},
  venue     = {Saint Petersburg, Russia},
  eventdate = {July 13-19, 2013},
  series    = lncs,
  volume    = {8044},
  publisher = {Springer},
  address   = {Berlin, Heidelberg},
  year      = {2013},
  doi       = {10.1007/978-3-642-39799-8},
  isbn      = {978-3-642-39798-1},
  timestamp = {Tue, 14 May 2019 10:00:43 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/cav/2013},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% AVATAR
@inproceedings{Voronkov2014,
abstract = {This paper describes a new architecture for first-order resolution and superposition theorem provers called AVATAR (Advanced Vampire Architecture for Theories and Resolution). Its original motivation comes from a problem well-studied in the past - dealing with problems having clauses containing propositional variables and other clauses that can be split into components with disjoint sets of variables. Such clauses are common for problems coming from applications, for example in program verification and program analysis, where many ground literals occur in the problems and even more are generated during the proof-search. This problem was previously studied by adding various versions of splitting. The addition of splitting resulted in some improvements in performance of theorem provers. However, even with various versions of splitting, the performance of superposition theorem provers is nowhere near SMT solvers on variable-free problems or SAT solvers on propositional problems. This paper describes a new architecture for superposition theorem provers, where a superposition theorem prover is tightly integrated with a SAT or an SMT solver. Its implementation in our theorem prover Vampire resulted in drastic improvements over all previous implementations of splitting. Over four hundred TPTP problems previously unsolvable by any modern prover, including Vampire itself, have been proved, most of them with short runtimes. Nearly all problems solved with one of 481 variants of splitting previously implemented in Vampire can also be solved with AVATAR. We also believe that AVATAR is an important step towards efficient reasoning with both quantifiers and theories, which is one of the key areas in modern applications of theorem provers in program analysis and verification. {\textcopyright} 2014 Springer International Publishing.},
author = {Voronkov, Andrei},
booktitle="Computer Aided Verification",
editor="Biere, Armin
and Bloem, Roderick",
series = lncs,
doi = {10.1007/978-3-319-08867-9\_46},
isbn = {9783319088662},
issn = {16113349},
pages = {696--710},
publisher = {Springer},
address = {Cham},
title = {{AVATAR}: The architecture for first-order theorem provers},
volume = {8559 LNCS},
year = {2014}
}

@article{DBLP:journals/logcom/BachmairG94,
  author    = {Leo Bachmair and
               Harald Ganzinger},
  title     = {Rewrite-Based Equational Theorem Proving with Selection and Simplification},
  journal   = {J. Log. Comput.},
  volume    = {4},
  number    = {3},
  pages     = {217--247},
  year      = {1994},
  doi       = {10.1093/logcom/4.3.217},
  timestamp = {Wed, 17 May 2017 14:25:56 +0200},
  biburl    = {https://dblp.org/rec/journals/logcom/BachmairG94.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{DBLP:books/daglib/0040158,
  author    = {Ian J. Goodfellow and
               Yoshua Bengio and
               Aaron C. Courville},
  title     = {Deep Learning},
  series    = {Adaptive computation and machine learning},
  publisher = {{MIT} Press},
  year      = {2016},
  url       = {http://www.deeplearningbook.org/},
  isbn      = {978-0-262-03561-3},
  timestamp = {Sat, 25 Mar 2017 20:16:59 +0100},
  biburl    = {https://dblp.org/rec/books/daglib/0040158.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{DBLP:books/el/RV01/NonnengartW01,
  author    = {Andreas Nonnengart and
               Christoph Weidenbach},
  title     = {Computing Small Clause Normal Forms},
  pages     = {335--367},
  crossref  = {DBLP:books/el/RobinsonV01},
  doi       = {10.1016/b978-044450813-3/50008-4},
  timestamp = {Thu, 25 Jul 2019 12:26:00 +0200},
  biburl    = {https://dblp.org/rec/books/el/RV01/NonnengartW01.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@book{DBLP:books/el/RobinsonV01,
  editor    = {John Alan Robinson and
               Andrei Voronkov},
  title     = {Handbook of Automated Reasoning (in 2 volumes)},
  publisher = {Elsevier and {MIT} Press},
  year      = {2001},
  isbn      = {0-444-50813-9},
  timestamp = {Thu, 18 Feb 2021 15:31:26 +0100},
  biburl    = {https://dblp.org/rec/books/el/RobinsonV01.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% This entry has no URL so that the URL is not typeset next to the chapters of this book, since the chapters are identified by DOIs.
% https://www.sciencedirect.com/book/9780444508133/handbook-of-automated-reasoning
% https://mitpress.mit.edu/books/handbook-automated-reasoning-volume-1

@inproceedings{DBLP:conf/cade/WeidenbachDFKSW09,
  author    = {Christoph Weidenbach and
               Dilyana Dimova and
               Arnaud Fietzke and
               Rohit Kumar and
               Martin Suda and
               Patrick Wischnewski},
  title     = {{SPASS} Version 3.5},
  pages     = {140--145},
  crossref  = {DBLP:conf/cade/2009},
  doi       = {10.1007/978-3-642-02959-2\_10},
  timestamp = {Tue, 14 May 2019 10:00:39 +0200},
  biburl    = {https://dblp.org/rec/conf/cade/WeidenbachDFKSW09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@proceedings{DBLP:conf/cade/2009,
  editor    = {Renate A. Schmidt},
  booktitle = {Automated Deduction - CADE-22},
  venue     = {Montreal, Canada},
  eventdate = {August 2-7, 2009},
  series    = lncs,
  volume    = {5663},
  publisher = {Springer},
  address   = {Berlin, Heidelberg},
  year      = {2009},
  doi       = {10.1007/978-3-642-02959-2},
  isbn      = {978-3-642-02958-5},
  timestamp = {Thu, 18 Feb 2021 15:25:24 +0100},
  biburl    = {https://dblp.org/rec/conf/cade/2009.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% GCN
@inproceedings{kipf2017semisupervised,
abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1609.02907},
author = {Kipf, Thomas N. and Welling, Max},
booktitle = {5th International Conference on Learning Representations, ICLR 2017},
eprint = {1609.02907},
month = sep,
title = {Semi-Supervised Classification with Graph Convolutional Networks},
url = {https://openreview.net/forum?id=SJU4ayYgl},
year = {2017}
}

% R-GCN
@inproceedings{Schlichtkrull2017,
  author    = {Michael Sejr Schlichtkrull and
               Thomas N. Kipf and
               Peter Bloem and
               Rianne van den Berg and
               Ivan Titov and
               Max Welling},
  editor    = {Aldo Gangemi and
               Roberto Navigli and
               Maria{-}Esther Vidal and
               Pascal Hitzler and
               Rapha{\"{e}}l Troncy and
               Laura Hollink and
               Anna Tordai and
               Mehwish Alam},
  title     = {Modeling Relational Data with Graph Convolutional Networks},
  booktitle = {The Semantic Web},
  venue     = {Heraklion, Crete, Greece},
  eventdate = {June 3-7, 2018},
  series    = lncs,
  volume    = {10843},
  pages     = {593--607},
  publisher = {Springer},
  address   = {Cham},
  year      = {2018},
  doi       = {10.1007/978-3-319-93417-4\_38},
  isbn="978-3-319-93417-4",
  timestamp = {Tue, 14 May 2019 10:00:44 +0200},
  biburl    = {https://dblp.org/rec/conf/esws/SchlichtkrullKB18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% ENIGMA-NG
@inproceedings{Chvalovsky2019,
archivePrefix = {arXiv},
arxivId = {1903.03182},
author = {Chvalovsk{\'{y}}, Karel and Jakubův, Jan and Suda, Martin and Urban, Josef},
crossref = {DBLP:conf/cade/2019},
doi = {10.1007/978-3-030-29436-6\_12},
eprint = {1903.03182},
month = mar,
title = {{ENIGMA-NG}: Efficient Neural and Gradient-Boosted Inference Guidance for {E}},
year = {2019}
}

% ENIGMA Anonymous
@inproceedings{Jakubuv2020,
abstract = {We describe an implementation of gradient boosting and neural guidance of saturation-style automated theorem provers that does not depend on consistent symbol names across problems. For the gradient-boosting guidance, we manually create abstracted features by considering arity-based encodings of formulas. For the neural guidance, we use symbol-independent graph neural networks (GNNs) and their embedding of the terms and clauses. The two methods are efficiently implemented in the E prover and its ENIGMA learning-guided framework. To provide competitive real-time performance of the GNNs, we have developed a new context-based approach to evaluation of generated clauses in E. Clauses are evaluated jointly in larger batches and with respect to a large number of already selected clauses (context) by the GNN that estimates their collectively most useful subset in several rounds of message passing. This means that approximative inference rounds done by the GNN are efficiently interleaved with precise symbolic inference rounds done inside E. The methods are evaluated on the MPTP large-theory benchmark and shown to achieve comparable real-time performance to state-of-the-art symbol-based methods. The methods also show high complementarity, solving a large number of hard Mizar problems.},
archivePrefix = {arXiv},
arxivId = {2002.05406},
author = {Jakubův, Jan and Chvalovsk{\'{y}}, Karel and Ol{\v{s}}{\'{a}}k, Miroslav and Piotrowski, Bartosz and Suda, Martin and Urban, Josef},
editor="Peltier, Nicolas
and Sofronie-Stokkermans, Viorica",
booktitle = {Automated Reasoning},
series = lncs,
doi = {10.1007/978-3-030-51054-1\_29},
eprint = {2002.05406},
isbn = {9783030510534},
issn = {16113349},
keywords = {Automated theorem proving,Decision trees,Machine learning,Neural networks,Saturation-style proving},
month = jul,
pages = {448--463},
publisher = {Springer},
address = {Cham},
title = {{ENIGMA} {A}nonymous: Symbol-Independent Inference Guiding Machine (System Description)},
volume = {12167 LNAI},
year = {2020}
}

% Layer Normalization
@misc{Ba2016,
abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
archivePrefix = {arXiv},
arxivId = {1607.06450},
author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
eprint = {1607.06450},
month = jul,
title = {Layer Normalization},
url = {http://arxiv.org/abs/1607.06450},
year = {2016}
}
% https://sites.google.com/site/nips2016deeplearnings/accepted-papers
% https://openreview.net/forum?id=BJLa_ZC9

% E
@inproceedings{Schulz2019,
abstract = {E 2.3 is a theorem prover for many-sorted first-order logic with equality. We describe the basic logical and software architecture of the system, as well as core features of the implementation. We particularly discuss recently added features and extensions, including the extension to many-sorted logic, optional limited support for higher-order logic, and the integration of SAT techniques via PicoSAT. Minor additions include improved support for TPTP standard features, always-on internal proof objects, and lazy orphan removal. The paper also gives an overview of the performance of the system, and describes ongoing and future work.},
author = {Schulz, Stephan and Cruanes, Simon and Vukmirovi{\'{c}}, Petar},
crossref = {DBLP:conf/cade/2019},
doi = {10.1007/978-3-030-29436-6\_29},
pages = {495--507},
title = {Faster, Higher, Stronger: {E} 2.3}
}

% LPO
@Unpublished{Kamin1980,
  author    = {Samuel N. Kamin and Jacques L{\'e}vy},
  title     = {Two generalizations of the recursive path ordering},
  year      = {1980},
  owner     = {filip},
  timestamp = {2019-12-02},
  note      = {Unpublished letter to Nachum Dershowitz},
  url       = {http://www.cs.tau.ac.il/~nachumd/term/kamin-levy80spo.pdf},
}

% KBO
@incollection{Knuth1983,
author="Knuth, D. E.
and Bendix, P. B.",
title="Simple Word Problems in Universal Algebras",
pages="342--376",
abstract="An algorithm is described which is capable of solving certain word problems: i.e. of deciding whether or not two words composed of variables and operators can be proved equal as a consequence of a given set of identities satisfied by the operators. Although the general word problem is well known to be unsolvable, this algorithm provides results in many interesting cases. For example in elementary group theory if we are given the binary operator {\textperiodcentered}, the unary operator −, and the nullary operator e, the algorithm is capable of deducing from the three identities a {\textperiodcentered} (b {\textperiodcentered} c) = (a {\textperiodcentered} b) {\textperiodcentered} c, a {\textperiodcentered} a− = e, a {\textperiodcentered} e = a, the laws a− {\textperiodcentered} a = e, e {\textperiodcentered} a = a, a− −= a, etc.; and furthermore it can show that a {\textperiodcentered} b = b {\textperiodcentered} a−is not a consequence of the given axioms.",
doi="10.1007/978-3-642-81955-1\_23",
crossref="Siekmann1983"
}
@book{Siekmann1983,
editor="Siekmann, J{\"o}rg H.
and Wrightson, Graham",
booktitle="Automation of Reasoning: 2: Classical Papers on Computational Logic 1967--1970",
year="1983",
publisher="Springer",
address="Berlin, Heidelberg",
isbn="978-3-642-81955-1"
}

% TKBO
@inproceedings{Ludwig2007,
abstract = {The Knuth-Bendix ordering is usually preferred over the lexicographic path ordering in successful implementations of resolution and superposition, but it is incompatible with certain requirements of hierarchic superposition calculi. Moreover, it does not allow non-linear definition equations to be oriented in a natural way. We present an extension of the Knuth-Bendix ordering that makes it possible to overcome these restrictions. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
author = {Ludwig, Michel and Waldmann, Uwe},
editor="Dershowitz, Nachum
and Voronkov, Andrei",
series = lncs,
booktitle="Logic for Programming, Artificial Intelligence, and Reasoning",
doi = {10.1007/978-3-540-75560-9\_26},
isbn = {9783540755586},
issn = {16113349},
month = oct,
pages = {348--362},
publisher = {Springer},
address = {Berlin, Heidelberg},
title = {An extension of the {K}nuth-{B}endix ordering with {LPO}-like properties},
volume = {4790 LNAI},
year = {2007}
}

% TKBO in Vampire
@inproceedings{Kovacs2011,
abstract = {In this paper we discuss the recently introduced transfinite Knuth-Bendix orders. We prove that any such order with finite subterm coefficients and for a finite signature is equivalent to an order using ordinals below $\omega$$\omega$, that is, finite sequences of natural numbers of a fixed length. We show that this result does not hold when subterm coefficients are infinite. However, we prove that in this general case ordinals below $\omega$$\omega$$\omega$ suffice. We also prove that both upper bounds are tight. We briefly discuss the significance of our results for the implementation of first-order theorem provers and describe relationships between the transfinite Knuth-Bendix orders and existing implementations of extensions of the Knuth-Bendix orders. {\textcopyright} 2011 Springer-Verlag Berlin Heidelberg.},
author = {Kov{\'{a}}cs, Laura and Moser, Georg and Voronkov, Andrei},
editor="Bj{\o}rner, Nikolaj
and Sofronie-Stokkermans, Viorica",
booktitle="Automated Deduction -- CADE-23",
doi = {10.1007/978-3-642-22438-6\_29},
isbn="978-3-642-22438-6",
issn = {03029743},
series = lncs,
pages = {384--399},
publisher = {Springer},
address = {Berlin, Heidelberg},
title = {On transfinite {K}nuth-{B}endix orders},
volume = {6803 LNAI},
year = {2011}
}

% invfreq
@Booklet{E-manual,
address = {Manchester},
author = {Schulz, Stephan},
issn = {2516-2314},
publisher = {EasyChair},
series = {EasyChair preprint},
title = {{E} 2.4 User Manual},
url = {https://easychair.org/publications/preprint/8dss},
year = {2020},
howpublished = {EasyChair preprint no. 2272},
number = 2272
}

% UCB1
@article{Auer2002,
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Fischer, Paul},
doi = {10.1023/A:1013689704352},
issn = {08856125},
journal = {Machine Learning},
keywords = {Adaptive allocation rules,Bandit problems,Finite horizon regret},
month = may,
number = {2-3},
pages = {235--256},
publisher = {Springer},
title = {Finite-time analysis of the multiarmed bandit problem},
volume = {47},
year = {2002}
}

% Adam
@misc{Kingma2014,
   abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
   author = {Diederik P. Kingma and Jimmy Ba},
   month = dec,
   title = {Adam: A Method for Stochastic Optimization},
   url = {http://arxiv.org/abs/1412.6980},
   year = {2014},
   eprint = {1412.6980},
   archivePrevix = {arXiv}
}

@book{Harrison2009,
address = {Cambridge},
author = {Harrison, John},
doi = {10.1017/CBO9780511576430},
isbn = {9780511576430},
publisher = {Cambridge University Press},
title = {Handbook of Practical Logic and Automated Reasoning},
year = {2009}
}

@book{Mohri2018,
author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
edition = 2,
isbn = {9781787284395},
publisher = {MIT Press},
title = {Foundations of Machine Learning},
url = {https://cs.nyu.edu/~mohri/mlbook/},
year = {2018}
}

% RankNet
@inproceedings{Burges2005,
abstract = {We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.},
address = {New York, New York, USA},
author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
booktitle = {ICML 2005 - Proceedings of the 22nd International Conference on Machine Learning},
doi = {10.1145/1102351.1102363},
isbn = {1595931805},
pages = {89--96},
publisher = {ACM Press},
title = {Learning to Rank using Gradient Descent},
year = {2005}
}

% Tseitin transformation
@incollection{Tseitin1983,
address = {Berlin, Heidelberg},
author = {Tseitin, G. S.},
booktitle = {Automation of Reasoning},
doi = {10.1007/978-3-642-81955-1\_28},
pages = {466--483},
publisher = {Springer Berlin Heidelberg},
title = {On the Complexity of Derivation in Propositional Calculus},
year = {1983}
}

% TPTP
@article{Sutcliffe2017,
author = {Sutcliffe, Geoff},
doi = {10.1007/s10817-017-9407-7},
issn = {0168-7433},
journal = {Journal of Automated Reasoning},
month = dec,
number = {4},
title = {The {TPTP} Problem Library and Associated Infrastructure},
volume = {59},
year = {2017}
}

% TPTP Syntax
@misc{TptpSyntax,
title = {{TPTP} Syntax},
key = {TPTP Syntax},
url = {http://www.tptp.org/TPTP/SyntaxBNF.html},
urldate = {2021-02-22}
}

% Heterogeneous information network (HIN)
@article{Shi2015,
archivePrefix = {arXiv},
arxivId = {1511.04854},
author = {Shi, Chuan and Li, Yitong and Zhang, Jiawei and Sun, Yizhou and Yu, Philip S.},
eprint = {1511.04854},
month = nov,
title = {A Survey of Heterogeneous Information Network Analysis},
url = {https://arxiv.org/abs/1511.04854},
year = {2015}
}

@inproceedings{Olsak2019,
  author    = {Miroslav Ol{\v{s}}{\'{a}}k and
               Cezary Kaliszyk and
               Josef Urban},
  editor    = {Giuseppe De Giacomo and
               Alejandro Catal{\'{a}} and
               Bistra Dilkina and
               Michela Milano and
               Sen{\'{e}}n Barro and
               Alberto Bugar{\'{\i}}n and
               J{\'{e}}r{\^{o}}me Lang},
  title     = {Property Invariant Embedding for Automated Reasoning},
  booktitle = {{ECAI} 2020 -- 24th European Conference on Artificial Intelligence},
  venue     = {Santiago de Compostela, Spain},
  eventdate = {August 29 - September 8, 2020},
  series    = {Frontiers in Artificial Intelligence and Applications},
  volume    = {325},
  pages     = {1395--1402},
  publisher = {{IOS} Press},
  year      = {2020},
  doi       = {10.3233/FAIA200244},
  timestamp = {Fri, 09 Apr 2021 18:50:06 +0200},
  biburl    = {https://dblp.org/rec/conf/ecai/OlsakKU20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Graph neural networks (GNNs)
@misc{Zhou2018,
abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive GNNs have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on variants of graph neural networks such as graph convolutional network (GCN), graph attention network (GAT), gated graph neural network (GGNN) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.},
archivePrefix = {arXiv},
arxivId = {1812.08434},
author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
eprint = {1812.08434},
month = dec,
title = {Graph Neural Networks: A Review of Methods and Applications},
url = {http://arxiv.org/abs/1812.08434},
year = {2018}
}

@proceedings{DBLP:conf/cade/2019,
  editor    = {Pascal Fontaine},
  title     = {Automated Deduction - {CADE} 27},
  series    = lncs,
  volume    = {11716},
  publisher = {Springer},
  address   = {Cham},
  year      = {2019},
  doi       = {10.1007/978-3-030-29436-6},
  isbn      = {978-3-030-29435-9},
  timestamp = {Mon, 03 May 2021 18:51:23 +0200},
  biburl    = {https://dblp.org/rec/conf/cade/2019.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
