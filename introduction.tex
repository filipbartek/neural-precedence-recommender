% !TEX root = main.tex

% General motivation for FOL ATPing:
% Vampire is used as a sledgehammer in Isabelle/HOL:
% 1. https://people.mpi-inf.mpg.de/~jblanche/life.pdf
% 2. https://www.cl.cam.ac.uk/~lp15/papers/Automation/paar.pdf
% Formal methods for verification (survey): https://arxiv.org/pdf/1912.03028.pdf

% Jones 2016 guidelines:
% 1. Problem (explain by example)
% 2. Contributions (refutable, forward references to sections)

% AWR structure (Unit 7, page 1):
% 1. Attention-getter (lead-in) [1-2 sentences]
% 2. Set up for the thesis [minimum: 2-3 sentences]
% 3. Thesis statement (essay map) [1 sentence]

% AWR guidelines (Unit 7, page 5):
% Whet the reader's appetite
% Set the context
% State why the main idea is important
% State your thesis/claim

Modern saturation-based Automatic Theorem Provers (ATPs) such as E \cite{Schulz2019}, SPASS \cite{DBLP:conf/cade/WeidenbachDFKSW09},
or \Vampire{} \cite{DBLP:conf/cav/KovacsV13}
employ the superposition calculus \cite{DBLP:journals/logcom/BachmairG94,DBLP:books/el/RV01/NieuwenhuisR01} as their underlying inference system.
Integrating the flavors of resolution \cite{DBLP:books/el/RV01/BachmairG01}, paramodulation \cite{Robinson1983}, and 
the unfailing completion \cite{Bachmair89completionwithout},
superposition is a powerful calculus with
native support for equational reasoning.
The calculus is parameterized by a simplification ordering on terms % and literals,
and uses it to constrain the applicability of inferences, with a significant impact on performance.

Both main classes of simplification orderings used in practice,
the \acrlong*{kbo} \cite{Knuth1983}
and the \acrlong*{lpo} \cite{Kamin1980},
are specified with the help of a 
\emph{symbol precedence}, an ordering on the signature symbols. %.\footnote{KBO is further parameterized by symbol weights.}
% but our reference implementation in \Vampire{}~\cite{DBLP:conf/cav/KovacsV13} 
% uses for efficiency reasons only weights equal to one \cite{DBLP:conf/cade/KovacsMV11} and so we do not consider this parameter here.}
While the superposition calculus is refutationally complete for any simplification ordering \cite{DBLP:journals/logcom/BachmairG94},
the choice of the precedence has a significant impact on how long it takes to solve a given problem.

It is well known that giving the highest precedence to the predicate symbols introduced as sub-formula names 
during clausification \cite{DBLP:books/el/RV01/NonnengartW01}
% during the Tseitin transformation % of the input formula \cite{Tseitin1983} 
can immediately make the saturation produce the exponential 
set of clauses that the transformation is designed to avoid \cite{Reger2016}.
Also, certain orderings help to make the superposition a decision procedure on specific fragments of first-order logic 
(see, e.g., \cite{DBLP:conf/lics/GanzingerN99,DBLP:conf/cade/HustadtKS05}).
However, the precise way by which the choice of a precedence 
influences the follow-up proof search on a general problem is extremely hard to predict. % indirect

% neslo by rict na tvrdo, ze je to takovy ten mytus o tom, ze se uzivatel zamysli a zvoli si tu skvelou precedenci pro svuj problem?

Several general-purpose precedence generating schemes are available to ATP users,
such as the successful \texttt{invfreq} scheme in E \cite{E-manual}, which orders the symbols 
by the number of occurrences in the input problem. However, experiments with random precedences
indicate that the existing schemes often fail to come close to the optimum precedence \cite{RegerSuda2017},
suggesting room for further improvements.

In this work, we propose a \acrlong{ml} system that learns to predict for an ATP
whether one precedence will lead to a faster proof search on a given problem than another.
Given a previously unseen problem, it can then be asked to recommend the best possible precedence for an ATP to run with.
Relying only on the logical structure of the problems, % for the learning, 
the system generalizes the knowledge about favorable precedences across problems with different signatures.

Our recommender uses a relational graph convolutional neural network \cite{Schlichtkrull2017}
to represent the problem structure. It learns from the ATP performance on selected problems
and pairs of randomly sampled precedences. This information is used to train
a \emph{symbol cost model}, which then realizes the recommendation by simply sorting 
the problem's symbols according to the obtained costs. 

This work strictly improves on our previous experiments with linear regression models and simple hand-crafted symbol features \cite{DBLP:conf/cade/Bartek020}
and is, to the best of our knowledge, the first method able to propose good symbol precedences automatically 
using a non-linear transformation of the input problem structure.

The rest of this paper is organized as follows.
\Cref{sec:preliminaries} exposes the basic terminology used throughout the remaining sections.
\Cref{sec:architecture} proposes a structure of the precedence recommender that can be trained on pairs of symbol precedences,
as described in \cref{sec:training}.
\Cref{sec:evaluation} summarizes and discusses experiments performed
using an implementation of the precedence recommender.
\Cref{sec:related} compares the system proposed in this work with notable related works.
\Cref{sec:conclusion} concludes the investigation and outlines possible directions for future research.

%\todo{MS: idea maybe to work out more in the introduction: 
%We could stress how the impact or the choice of a precedence is \emph{indirect}, 
%(as it's already obvious from the explanation here)
%all the more interesting one can learn from observing just this indirect impact 
%which precendences are good and which are bad!}

% Two aspects: 
% -selection of \emph{maximals} 
% -rewriting from \emph{large to small}
