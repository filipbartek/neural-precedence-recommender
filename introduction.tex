% !TEX root = main.tex

% General motivation for FOL ATPing:
% Vampire is used as a sledgehammer in Isabelle/HOL:
% 1. https://people.mpi-inf.mpg.de/~jblanche/life.pdf
% 2. https://www.cl.cam.ac.uk/~lp15/papers/Automation/paar.pdf
% Formal methods for verification (survey): https://arxiv.org/pdf/1912.03028.pdf

% Jones 2016 guidelines:
% 1. Problem (explain by example)
% 2. Contributions (refutable, forward references to sections)

% AWR structure (Unit 7, page 1):
% 1. Attention-getter (lead-in) [1-2 sentences]
% 2. Set up for the thesis [minimum: 2-3 sentences]
% 3. Thesis statement (essay map) [1 sentence]

% AWR guidelines (Unit 7, page 5):
% Whet the reader's appetite
% Set the context
% State why the main idea is important
% State your thesis/claim

Modern saturation-based Automatic Theorem Provers (ATPs) such as E \cite{SCV:CADE-2019}, SPASS \cite{DBLP:conf/cade/WeidenbachDFKSW09} 
or \Vampire{} \cite{DBLP:conf/cav/KovacsV13}
employ the superposition calculus \cite{DBLP:journals/logcom/BachmairG94,DBLP:books/el/RV01/NieuwenhuisR01} as their underlying inference system.
Integrating flavours of resolution \cite{DBLP:books/el/RV01/BachmairG01}, paramodulation \cite{Robinson1983}, and 
the unfailing completion \cite{Bachmair89completionwithout}, superposition is a powerful calculus with 
a native support for equational reasoning. The calculus is parametrized by a simplification ordering on terms % and literals,
and uses it to constrain the applicability of inferences, with a significant impact on performance.

Both of the two main classes of simplification orderings used in practice,
the \acrlong*{kbo} \cite{Knuth1983}
and the \acrlong*{lpo} \cite{Kamin1980},
are specified with the help of a 
\emph{symbol precedence}, an ordering on the signature symbols. %.\footnote{KBO is further parameterized by symbol weights.}
% but our reference implementation in \Vampire{}~\cite{DBLP:conf/cav/KovacsV13} 
% uses for efficiency reasons only weights equal to one \cite{DBLP:conf/cade/KovacsMV11} and so we do not consider this parameter here.}
While the superposition calculus is refutationally complete for any simplification ordering \cite{DBLP:journals/logcom/BachmairG94},
the choice of the precedence has a significant impact on how long it takes to solve a given problem.

Certain orderings help to make superposition a decision procedure on certain fragments of first-order logic 
(see, e.g., \cite{DBLP:conf/lics/GanzingerN99,DBLP:conf/cade/HustadtKS05}).
It is also well known that giving the highest precedence to the predicate symbols introduced as names 
during the Tseitin transformation % of the input formula 
\cite{Tseitin1983} can immediately make saturation produce the exponential 
set of clauses that the transformation is designed to avoid \cite{Reger2016}.
%
However, the precise way by which the choice of a precedence 
influences the follow-up proof search on a general problem is extremely hard to predict. % indirect

% neslo by rict na tvrdo, ze je to takovy ten mytus o tom, ze se uzivatel zamysli a zvoli si tu skvelou precedenci pro svuj problem?

Several general-purpose precedence generating schemes are available to ATP users,
such as the successful \texttt{invfreq} scheme in E \cite{E-manual}, which orders the symbols 
by the number of occurrences in the input problem. However, experiments with random precedences
indicate that the existing schemes often fail to come close to the optimum precedence \cite{RegerSuda2017},
suggesting room for further improvements.

In this work, 

\newpage


%\todo{MS: idea maybe to work out more in the introduction: 
%We could stress how the impact or the choice of a precedence is \emph{indirect}, 
%(as it's already obvious from the explanation here)
%all the more interesting one can learn from observing just this indirect impact 
%which precendences are good and which are bad!}


% Two aspects: 
% -selection of \emph{maximals} 
% -rewriting from \emph{large to small}


% CITE us: co tehdy, co ted!



Choosing the next inference to apply is arguably the most important decision point in the state-of-the-art
\todo{FB: Add "saturation-based"?}
\glspl{atp} for \gls{fol}.
Since the most popular \glspl{atp} use the superposition calculus with literal selection \cite{DBLP:journals/logcom/BachmairG94},
the \gls{sot}, such as the \gls{kbo} \cite{Knuth1983},
plays an important role in restricting the inferences and thus guiding the proof search.
The \gls{kbo} is typically specified by a symbol precedence,
that is a permutation of non-logical symbols of the input problem.
Each of the commonly used precedence heuristics orders the symbols
lexicographically according to a small number of simple
\todo{FB: Is using the "simple" terminology from PAAR paper better than trying to find a better term?}
symbol properties,
such as the number of occurrences in the input problem,
% vampire --symbol_precedence frequency
the arity of the symbol, or
% vampire --symbol_precedence arity
%occurrence in the conjecture,
% vampire --symbol_precedence_boost goal
%number of occurrences in unit clauses, or
% vampire --symbol_precedence_boost units
whether or not the symbol was introduced in the process of Tseitin transformation \cite{Tseitin1983} or Skolemization \cite{Harrison2009}.
\todo{FB: Is citing Harrison better than nothing?}
% vampire --introduced_symbol_precedence top
Given that each of the heuristics used in practice, up to our knowledge, fails on some problems,
\todo{Mention that we tried already to find a good simple heuristic by ML.}
the general task of
finding a symbol precedence that guides the proof search efficiently on an arbitrary problem
remains to be intriguing.
This paper explores the possibility of tackling this task by training a neural network
on the results of running the \gls{atp} \Vampire{} \cite{10.1007/978-3-642-39799-8_1} with randomly sampled precedences.
\todo{Ensure that the first paragraph fits on the first page.}

\todo[author=Filip,inline]{Consider an alternative approach: Start with an example problem, rewrite in two ways, describe how do state-of-the-art provers differentiate between these two ways (by symbol precedence), ...}

\todo[inline,author=FB]{Try to use the first page for something more flashy than exposure of saturation-based automated proving. Ideas: catchy puzzle-like example, abstract description of the crucial challenge etc.}

Modern \gls{fol} \glspl{atp}, such as \Vampire{}, E or SPASS,
solve \gls{fol} problems by iteratively applying the inference rules of superposition calculus.
Literal selection and equation orienting are employed to limit the ... while preserving the refutation completeness.
Both literal selection and equation orienting are determined by \gls{sot}.

A \gls{fol} problem consists of \gls{fol} formulas divided into two categories:
one or more premises and one conjecture.
% We assume that \gls{fol} is expanded to FOL.
% For this reason the article is "an".
When presented with a \gls{fol} problem,
a \definiendum{saturation-based} \gls{atp} negates the conjecture (as a first step of proof by contradiction)
and converts all the formulas into an equisatisfiable set of clauses in \gls{cnf}.
\todo[author=Filip]{Are there any saturation-based ATPs that don't use CNF?}
The prover proceeds to iteratively infer new provable clauses
until a contradiction, represented by the empty clause, is inferred,
or until no new clauses can be inferred.
A proof of the fact that the premises and the negated conjecture entail a contradiction
constitutes a proof that the premises entail the conjecture.
\todo[author=Filip]{Cite some source that explains refutation-based automated theorem proving. Perhaps Harris?}
\todo[author=Martin]{prvni odstavec uvodu bych pro CADE publikum vynechal a zacal rovnou s druhym}

The state-of-the-art \gls{fol} \glspl{atp},
such as \Vampire{} \cite{10.1007/978-3-642-39799-8_1} and E \cite{10.1007/978-3-030-29436-6_29},
are saturation-based and
infer new provable clauses by applying the rules of the \definiendum{superposition calculus}.
\todo[author=Filip]{Cite some source that explains superposition calculus.}
The inferences under superposition calculus are constrained by a \definiendum{simplification ordering on the terms}.
\todo[author=Filip]{Do we mean terms in sensu stricto (applications of functions), or generalized terms (applications of functions and predicates)?}
The two classes of simplification orderings commonly used in practice are
\gls{lpo} \cite{Kamin1980} and \gls{kbo} \cite{Knuth1983}.
In both classes the ordering is specified by a \definiendum{symbol precedence},
\todo[author=Filip]{This may be an oversimplification: KBO is further parameterized by symbol weights.}
which is a permutation of predicate and function symbols of the input problem.

The typical approach to constructing a symbol precedence orders the symbols by a property
that is relatively cheap and straightforward to compute,
such as the frequency (the number of occurrences in the input problem) \cite{E-manual} or the arity.
\todo[author=Filip]{Cite the original source of the arity heuristic.}
The structure of the problem contains much information that is relevant for the choice of symbol precedence,
but is completely ignored by the symbol properties that inform the symbol precedences.
\todo[author=Filip]{Why do we believe this?}
\todo[author=Martin]{Klicovou vetu ... by asi chtelo rozvest treba do celeho odstavce. Takhle vyzni dost krypticky a neni moc jasne, co se snazi vyjadrit: "The structure of the problem" neni predem vysvetleno. "completely ignored" - "ignored" trosku zavani nejakym agentem, pritom je zde predmetem neziva vec / abstraktum}

This paper describes the architecture (\cref{sec:architecture}) and an empirical evaluation (\cref{sec:evaluation})
of a symbol precedence heuristic based on a \gls{gcn} \cite{kipf2017semisupervised}
and trained on proof attempts with random symbol precedences.
The experiments presented in \cref{sec:evaluation} demonstrate an improvement of success rate by 20 \%
compared to the state-of-the-art heuristic.
\todo[author=Martin]{Poslednimyu ostaveci by taky asi jeste mohlo neco predhazet, aby to mohl shrnout: "the architecture" ma urcity clen, ale predtim zadna "architecture" jeste neni. "graph convolution network" taky vlastne zminujes poprve}

% Filip: I decided not to follow the AWR structure in favor of Martin and CADE.

Our contributions:
\begin{itemize}
\item Up to our knowledge, \cref{sec:architecture} constitutes the first attempt
to propose good symbol precedences automatically using a non-linear transformation of the input problem structure.
Specifically, we explore the possibility of using a \gls{gnn} to learn symbol representations conducive for creating good precedences.
\item The experimental evaluation in \cref{sec:evaluation} proves practical feasibility of the proposed approach.
\item Besides tackling the particular task of generating symbol precedences,
the approach described in \cref{sec:architecture} may be of a more general interest
for a researcher trying to apply statistical learning to produce arbitrarily long permutations
that optimize some non-differentiable quantitative criterion.
\todo{FB: Give examples of such tasks. How about constructive TSP?}
\end{itemize}

\iffalse % AWR draft

The performance of the state-of-the-art \acrfullpl{atp} is severely limited by some of the design decisions that constitute the provers' architecture.
For example, the \frequency{} symbol precedence heuristic of the \acrfull{fol} \acrshort{atp} \Vampire{} is easily outperformed by the best of 10 random precedences.\cite{}
The gap in performance suggests that a more elaborate precedence heuristic with a superior performance exists.
% what is precedence heuristic?
% Thesis statement
Training a model from proof attempts with random precedences yields a heuristic that significantly outperforms the state-of-the-art heuristic.

\fi
