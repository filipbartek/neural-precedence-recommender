----------------------- REVIEW 1 ---------------------
SUBMISSION: 94
TITLE: Neural Precedence Recommender
AUTHORS: Filip Bártek and Martin Suda

----------- Overall evaluation -----------
The paper presents in fairly good detail a neural network learning system for
proposing good symbol orderings for the superposition calculus. It then
shows that Vampire using the learned orderings outperforms the baseline
Vampire.

The paper is impressive: performing machine learning for
automated theorem proving has so far proved to be notoriously hard
and new well-explained results in this direction are very welcome. Also, the
level of detail is sufficient for getting a good insight, which
is not too common for really complex methods.

The most promising recent approach has been learning clause selection
by Enigma, giving impressive gains. The current paper focuses on learning
a different aspect: symbol orderings.

An obvious question is how is this work related to the work and methods
used for Enigma?

Although the paper related to the Enigma system by Jakubuv has been
cited, it appears to not be referred to in the text. It should be referred and
the reference list should be checked.

Despite the clear improvement over baseline, the percentage gained does not appear
to be too big. It is possible, that the potential effect of better orderings is not
too high, regardless of the optimizations. Can you give some insights for this
line of thought?  We should also hope for further improvements on the basis of
the impressive work presented.

Although a paper does a good job of presenting a lot of complex details
in a limited-size paper, a few sections would benefit from further clarification,
for example:

"Each layer consists of a collection of differentiable modules--one module per edge type." Are there just so many nodes in the layer as the types of graph edges in Figure 2, or is each edge in the whole graph (a large set) represented by a collection of modules?

"In our precedence recommender, we use a GCN-based symbol cost model c in place of the simple
symbol property, ..." What exactly is the GCN-bases symbol cost? Is it the cost calculated by
the model for each symbol, or are different copies of symbols in different occurrences given
different costs?


----------------------- REVIEW 2 ---------------------
SUBMISSION: 94
TITLE: Neural Precedence Recommender
AUTHORS: Filip Bártek and Martin Suda

----------- Overall evaluation -----------
This paper introduces a tool to automatically suggest a precedence of symbols for the Vampire theorem prover using the Knuth-Bendix Order. This tool is build using a graph convolutional neural network (GCN), that accept graphs representing a CNF formula as inputs. This GCN outputs a weight for each symbol occurring in the input formula, and sorting the symbols in a non-increasing order on these weights produces a recommended precedence. The learning of a precedence of predicates and function symbols is done independently.

  The method used to train the GCN is explained and the performances of the recommender are evaluated on TPTP FOF problems. The best results were obtained by combining both the predicate and function symbols GCN recommenders, in which case a substantial of improvement in terms of the number of problems solved (around 180 more problems solved) is observed.

  This paper does a good job at presenting both superposition theorem proving and GCN learning at an abstract enough level that researchers in both domains can get an idea of what is going on in the other domain. The process of training the GCN is also clearly described. Moreover, the gain in performance is substantial which, to my knowledge, is a first as far as ML-based techniques for theorem proving are concerned. For all these reasons, I believe this paper should be accepted at CADE 2021, although the analysis of the key patterns used by the trained recommender (only hinted at as a future work in the conclusion) would have been much more interesting in my opinion.

  Detailed comments
  -----------------

 - page 4: Why present only the feedforward NN in sect. 2.3 when you are using a GCN? You could at least include a paragraph about the main differences between these two models at the end of 2.3. Then 3.2 could focus on the aspects of GCN that are specific to this particular problem.
 - p5: (Fig. 1) The small bold arrows are quite difficult to distinguish from the small non-bold arrows.
 - p6-7: (Fig. 2-3) There are unneeded differences in the notations used in the two figures; for example, pos/neg vs +/-, equality vs equality atom, the various (and non-specified) types of boxes around the nodes in fig. 3...
 - p9: In the equations showing that the expected value of the precedence cost is independent of the size of the signature, there is a (minor) problem in the second line: the i occurring in E_i[c(i)] is not defined and probably unneeded if, as hinted by the equations, the expected value of each symbol is the same (which should also be stated somewhere).
 - p10: In "This enables the use of gradient descent to find the values of the parameters of c that minimize the loss value.", I expect that you mean "to *locally* minimize the loss value". If you do, the fix is simple (just add "locally"); if not, more explanations are needed.
 - p11: the reference for Vampire ([20]) describes version 2.6 of Vampire and you are using version 4.3. If available, something more recent would give a better idea of the current capacities of Vampire.
 - p11: Did you fix the strategy and heuristics used by Vampire throughout the training and experiments or was it run in portfolio mode? What about AVATAR, was it activated?
 - p12: Regarding the sampling of problems, what happens when n_P = 0, i.e., a problem has not yet been sampled? Or do you start with trying each problem at least once before implementing UCB1?
 - p12: To clarify the last paragraph of the page, that introduces concepts with which people outside of ML wouldn't be familiar with, I suggest to use columns to show that some sentences are definitions for the terms introduced in the previous sentences, in particular, "... and 0 otherwise. Adaptive sampling..." -> "... and 0 otherwise: Adaptive sampling..." and "... with a parallelizing relaxation. In each iteration..." -> "with a parallelizing relaxation: In each iteration..."
 - p13: Same comment as just above: "... Adam optimizer [15]. Learning rate..." -> "... Adam optimizer [15]: Learning rate..."
 - p14: What about the remaining 20% that do not take at most 1.26 seconds? Do they all timeout?
 - p14: In your PAAR paper [7], the experimental results show that your linear model is not as good as the baseline but it is no longer the case here. How do you explain this? If it was further improved after PAAR, it should be mentioned.

  Minor comments
  --------------

 - p2: "according the obtained costs" -> "according to the obtained costs"
 - p3: "The inferences of superposition calculus" -> "The inferences of the superposition calculus"
 - p7: "most notably clausification" -> "most notably during clausification"
 - p9: (Lemma 1) "Precedence cost C" -> "The precedence cost C"
 - p9: (Lemma 1) define argmin
 - p12: "of Bernoulli multi-armed bandit problem" -> "of the Bernoulli multi-armed bandit problem"
 - p13: "Learning rate started" -> "The learning rate started"
 - p14: "a predicate and a function precedence was" -> "a predicate and a function precedences were"
 - p14: "at most at most"
 - p14: I assume the converse from the sentence "Since the predicate precedence recommender ... irrespective of the function precedence heuristic it is combined with." holds, in which case, you could add "and conversely" afterwards.
 - p16-17: The titles of some articles (mostly the arXiv ones, book titles are a different matter) are capitalized, while the others are not. This could be made uniform.
 - p17: There appears to be a latex bug in the url in reference 22.
----------- Specific questions for rebuttal -----------
- How does your tool behave in the presence of theories (assuming AVATAR is used)?
 - Now Vampire can also handle higher-order problems. Can your technique also improve Vampire's performance in this context?
 - How long did it take to do all the training?
 - What kind of new problems were solved in terms of the rating of the problems?
 - How independent are the training and test sets, given that the problems in TPTP are grouped by families that may include extremely similar problems?


----------------------- REVIEW 3 ---------------------
SUBMISSION: 94
TITLE: Neural Precedence Recommender
AUTHORS: Filip Bártek and Martin Suda

----------- Overall evaluation -----------
The paper describes an approach to learn good, problem-specific
  symbol precedences for the Knuth-Bendix-Order of a theorem prover
  based on superposition (where the ordering is both used to restrict
  the choice of inference literals in a clause, and to orient
  equational literals).

  The approach converts the problem to clause normal form (cnf) and
  represent the CNF as directed acyclic graph with perfect sharing of
  substructure. Each node type in this graph is associated with a
  numerical vector (with vectors at leaf nodes encoding some symbol
  properties upfront, but also containing trainable elements), and
  with a neural network translating incoming messages into an output
  value. The learned representation of each function- and predicate
  symbol is fed through a  single-layer network to produce a single
  value that is used to rank the symbol in the precedence. Precedences
  are ranked based on the performance of Vampire on the problem with
  the selected precedence. The loss function for training is computed
  by comparing numerical representations of the two precedences.

  The network is then trained on a large set of training examples with
  randomly generated total precedences, and used to suggest new
  precedences (or, more precisely, ranking weights for individual
  symbols). Overall, the approach leads a moderate improvements in the
  number of problems solved (compared to a good standard precedence
  generating scheme).

  I find the paper interesting and reasonably clear. There are a
  number of shortcomings (see below) that may explain the relatively
  modest success. But it is a good and solid first step.

  Detailed comments:

  - p1 (and elsewhere): Please check the paper for proper use of
    articles. In the case of superposition, it has "native support",
    not "a native support". There are similar problems (too many or
    too few articles) throughout the paper.
  - p2: The Tseitin transformation is a propositional procedure that
    introduces a definition for each complex subformula. While that is
    a similar idea as used in clausification of first-order formulas
    with definitions, the term does not extend to cover this.
  - p3: "As soon as the empty clause, denoted by [], is inferred, the
    prover concludes that the premises entail the conjecture, the
    sequence of inferences taken constituting a proof." Make this into
    two sentences. Also, is the proof the sequence of all inferences,
    or only the inferences contributing to the proof?
  - p3: The KBO allows both partial orderings and quasi-orderings for
    the precedence, so describing a precedence as a permutation of the
    symbol sequence is not, in general possible. Some implementations
    only support total orderings, but that is not a theoretical
    requirement, and should be made explicit.
  - p4/3 Architecture: Why do you use the number or iterations of the
    main loop? It's quite typical for calculus refinements to make the
    search space narrower, but deeper. In other words, a good
    precedence my well requite more iterations of the main loop and
    still find a proof much faster, because each iteration comprises
    much less work.
    You don't describe how many "incomplete" precedence pairs you
    have, e.g. pair where one precedence leads to a proof, the other
    to a timeout. This may be a more significant source of good
    training cases than the number of main loop iterations if both
    terminate within the time limit.
  - p13: "Next, both training and validation sets were restricted to
    problems whose graph representation consisted of at most 100 000
    nodes to limit the memory requirements of the training." If I
    understand you correctly, the graph representation of a problem is
    fixed. So why don't you filter before generating training
    examples? CPU time must be cheap! ;-)
  - p15: "KBO, that we choose in our experiments, is in addition to a
    symbol precedence also determined by symbol weights." -> The
    preceding sentence no grammar!
  - p16: The paper is technically over length, but only by the
    acknowledgements section (and would have the same number of total
    pages without it).
  - References: Your references 19 and 20 refer to the same paper with
    two slightly different BibItems.
  - The E 2.4 User Manual is available as an EasyChair Preprint (see
    below).


@Booklet{Schulz:EMANUAL-2019,
  author =       {Stephan Schulz},
  title =        {{E 2.4 User Manual}},
  howpublished = {EasyChair preprint no. 2272},
  url =          {https://easychair.org/publications/preprint/RjDx},
  year =         {2019},
}
----------- Specific questions for rebuttal -----------
See above.

